{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc4\n",
      "2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLPdata = pd.read_csv('nlpdata.csv')\n",
    "NLPdata = NLPdata.to_numpy()\n",
    "#X1train = NLParray[:,0].reshape((-1,1))\n",
    "#X2train = NLParray[:,1].reshape((-1,1))\n",
    "#Ytrain = NLParray[:,2]\n",
    "#print(X1train)\n",
    "#print(X2train)\n",
    "#print(Ytrain)\n",
    "\n",
    "#NLPdata = pd.read_csv('nlpdata.csv')\n",
    "#NLPdata = NLPdata.to_numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Original testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_dim=1)) \n",
    "model.add(Dense(28, activation='relu')) \n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  [4. 4. 5. 4. 4. 4. 4. 4. 3. 1. 4. 4. 3. 4. 4. 4. 4. 4. 4. 2. 4. 4. 4. 3.\n",
      " 1. 2. 3. 1. 4. 4. 6. 4. 4. 1. 4. 4. 1. 4. 4. 2. 3. 4. 4. 2. 1. 5. 3. 2.\n",
      " 5. 4. 4. 2. 4. 4. 1. 1. 4. 4. 2. 4. 2. 3. 2. 4. 4. 4. 4. 4. 4. 3. 6. 3.\n",
      " 3. 4. 3. 4. 2. 5. 2. 1. 4. 1. 4. 3. 4. 1. 4. 1. 3. 1. 3. 3. 3. 4. 4. 5.\n",
      " 4. 4. 3. 4. 4. 4. 4. 4. 2. 2. 4. 4. 4. 4. 4. 1. 2. 3. 2. 4. 4. 4. 1. 4.\n",
      " 5. 1. 5. 4. 4. 2. 4. 3. 2. 4. 3. 3. 4. 1. 3. 5. 3. 2. 3. 5. 4. 4. 4. 4.\n",
      " 3. 1. 4. 3. 2. 2. 4. 2. 4. 4. 4. 4. 2. 4. 4. 3. 4. 1. 4. 4. 4. 3. 4. 4.\n",
      " 2. 2. 2. 4. 4. 3. 4. 3. 4. 4. 2. 2. 4. 4. 5. 1. 2. 4. 4. 2. 2. 4. 3. 4.\n",
      " 4. 4. 5. 4. 2. 3. 1. 4. 3. 4. 4. 3. 2. 4. 1. 4. 4. 2. 4. 4. 4. 4. 4. 4.\n",
      " 2. 2. 4. 2. 4. 4. 4. 5. 4. 3. 4. 4. 3. 2. 1. 3. 4. 4. 3. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 1. 4. 4. 3. 1. 3. 4. 4. 4. 4. 4. 4. 2. 4. 2. 3.\n",
      " 3. 4. 2. 4. 4. 4. 4. 4. 1.]\n",
      "y_train:  [ 0.05        0.15918367  0.          0.16241497  0.          0.15642857\n",
      " -0.09270833 -0.06452991  0.01012397  0.43333333 -0.02916667  0.109375\n",
      "  0.05333333  0.1        -0.01458333  0.0795068   0.09        0.17857143\n",
      "  0.275       0.06009091  0.2625     -0.01916667  0.09166667  0.27559524\n",
      "  0.13318182  0.5         0.19622024  0.          0.2         0.21785714\n",
      "  0.25       -0.10833333  0.15625     0.07666667  0.2625      0.22222222\n",
      "  0.0625     -0.08636364  0.18083333  0.          0.09970238  0.22166667\n",
      "  0.095       0.08848485  0.09333333  0.07238095 -0.175       0.03541667\n",
      "  0.17424242  0.11        0.09583333  0.16805556  0.06041667  0.19410173\n",
      "  0.          0.28541667  0.25        0.16369048  0.14166667  0.21875\n",
      "  0.2         0.18050964  0.29166667  0.         -0.02588745  0.23224638\n",
      "  0.          0.125       0.07935606  0.15        0.25        0.025\n",
      "  0.12       -0.078125   -0.071875   -0.08518518 -1.          0.14895833\n",
      "  0.01333333  0.2125      0.44        0.0125      0.06944444  0.12857143\n",
      "  0.04375     0.23459208  0.13928571  0.6         0.515      -0.13645833\n",
      "  0.25208333  0.28928571  0.13125     0.10476191  0.2         0.22857143\n",
      "  0.21808036  0.5         0.01882716  0.40333333  0.1         0.45833333\n",
      "  0.18095238  0.04791667  0.07058823  0.06924242  0.02222222  0.03629704\n",
      " -0.15555556 -0.05        0.23579545 -0.5         0.00449495  0.16666667\n",
      "  0.          0.13863636 -0.034375    0.         -0.66666667  0.14583333\n",
      "  0.10871212  0.15659341 -0.02630386 -0.00779221  0.6        -0.125\n",
      "  0.13333333  0.25        0.06966667  0.1         0.0375      0.00595238\n",
      "  0.01871693  0.10548611  0.259375    0.          0.2         0.11470058\n",
      "  0.06471861  0.1694697   0.20558036  0.52        0.06595238  0.075\n",
      "  0.22857143 -0.03333333  0.06666667 -0.01458333  0.1527972   0.11071429\n",
      "  0.          0.          0.05277778 -0.046875   -0.00333333  0.18229167\n",
      "  0.0476087   0.28214286  0.0875     -0.07777778  0.26944444  0.01\n",
      " -0.04466667  0.14404762  0.45        0.09761905  0.175      -0.03333333\n",
      "  0.45        0.06428571  0.1         0.20767196  0.          0.\n",
      "  0.26346154  0.14        0.45833333 -0.125       0.11583333  0.00476431\n",
      "  0.17777778  0.15833333 -0.14861111 -0.16666667  0.06944444  0.20857143\n",
      "  0.05404762 -0.04378157  0.15170068  0.125       0.08374242  0.16666667\n",
      "  0.          0.25        0.25909091  0.185       0.03592558  0.22777778\n",
      "  0.11308081  0.17291667 -0.25       -0.65       -0.11777778  0.02380952\n",
      "  0.          0.30833333 -0.015       0.09393939  0.0625      0.13809524\n",
      "  0.15333333 -0.04356061  0.09402597  0.2147549   0.3         0.20189349\n",
      "  0.28333333 -0.35       -0.1075      0.45        0.10555556  0.3\n",
      "  0.07052083  0.1225      0.17083333  0.2         0.06428571  0.02579365\n",
      " -1.          0.25        0.28583333  0.16176471  0.23333333  0.01369048\n",
      " -0.45        0.27575758  0.21666667  0.01111111  0.08416667 -0.01333333\n",
      "  0.19284512  0.16190476  0.01715007  0.01875     0.15        0.\n",
      "  0.04785902  0.20166667  0.17142857  0.28958333 -0.04523809 -0.01464646\n",
      "  0.0625      0.02166667  0.4        -0.03916667  0.5        -0.55\n",
      "  0.02060606  0.10833333  0.14526515  0.275       0.16666667  0.55\n",
      "  0.05552083 -0.06372549  0.          0.         -0.01277778  0.28888889\n",
      "  0.375      -0.31833333  0.46666667]\n",
      "X_test:  [3. 3. 4. 2. 4. 3. 4. 2. 3. 4. 3. 3. 5. 4. 4. 4. 4. 4. 4. 2. 1. 2. 4. 4.\n",
      " 4. 4. 4. 1. 4. 2. 2. 4. 4. 3. 1. 2. 2. 1. 2. 2. 1. 2. 3. 4. 4. 4. 4. 4.\n",
      " 4. 2. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 3. 3. 1. 3. 4. 4. 2. 4. 3. 2. 4.\n",
      " 3. 5. 4. 2. 1. 3. 4. 4. 4. 4. 3. 4. 4. 5. 3. 1. 2. 4. 1. 4.]\n",
      "y_test:  [ 0.275       0.00555556  0.13076923  0.10504329  0.25694444  0.09285714\n",
      " -0.04166667  0.125       0.09675698  0.5        -0.01186526  0.26666667\n",
      "  0.06435185  0.19027778 -0.4         0.18450635  0.15        0.24404762\n",
      "  0.34545455 -0.025       0.2125      0.1030303   0.44444444 -0.55555556\n",
      "  0.4         0.2765873   0.13026245  0.02453704  0.15948162  0.33888889\n",
      "  0.08458333  0.20160256  0.121875    0.19820513  0.06428571  0.09419643\n",
      "  0.19888889  0.1537037   0.15208333  0.          0.38333333 -0.2\n",
      "  0.28571429 -0.225       0.33333333 -0.29166667  0.05769231  0.01944444\n",
      "  0.675       0.09545455 -0.17638889  0.46666667  0.07967607 -0.2\n",
      "  0.          0.         -0.06527778  0.21645833  0.2575      0.12380952\n",
      "  0.01636905  0.          0.18333333  0.25        0.1625      0.56666667\n",
      "  0.155       0.44        0.3        -0.01214286  0.07142857 -0.3625\n",
      "  0.0625     -0.31666667  0.47916667  0.06666667 -0.20833333  0.\n",
      "  0.17381245  0.07795056 -0.14583333  0.03125     0.04494949  0.11909091\n",
      "  0.07944444 -0.0625     -0.1         0.01136364  0.10277778  0.02777778\n",
      "  0.          0.08863636]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection as model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(NLPdata[:,0], NLPdata[:,2], train_size=0.75,test_size=0.25, random_state=101)\n",
    "print (\"X_train: \", X_train)\n",
    "print (\"y_train: \", y_train)\n",
    "print (\"X_test: \", X_test)\n",
    "print (\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  [[4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [6.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]]\n",
      "y_train:  [[ 0.05      ]\n",
      " [ 0.15918367]\n",
      " [ 0.        ]\n",
      " [ 0.16241497]\n",
      " [ 0.        ]\n",
      " [ 0.15642857]\n",
      " [-0.09270833]\n",
      " [-0.06452991]\n",
      " [ 0.01012397]\n",
      " [ 0.43333333]\n",
      " [-0.02916667]\n",
      " [ 0.109375  ]\n",
      " [ 0.05333333]\n",
      " [ 0.1       ]\n",
      " [-0.01458333]\n",
      " [ 0.0795068 ]\n",
      " [ 0.09      ]\n",
      " [ 0.17857143]\n",
      " [ 0.275     ]\n",
      " [ 0.06009091]\n",
      " [ 0.2625    ]\n",
      " [-0.01916667]\n",
      " [ 0.09166667]\n",
      " [ 0.27559524]\n",
      " [ 0.13318182]\n",
      " [ 0.5       ]\n",
      " [ 0.19622024]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.21785714]\n",
      " [ 0.25      ]\n",
      " [-0.10833333]\n",
      " [ 0.15625   ]\n",
      " [ 0.07666667]\n",
      " [ 0.2625    ]\n",
      " [ 0.22222222]\n",
      " [ 0.0625    ]\n",
      " [-0.08636364]\n",
      " [ 0.18083333]\n",
      " [ 0.        ]\n",
      " [ 0.09970238]\n",
      " [ 0.22166667]\n",
      " [ 0.095     ]\n",
      " [ 0.08848485]\n",
      " [ 0.09333333]\n",
      " [ 0.07238095]\n",
      " [-0.175     ]\n",
      " [ 0.03541667]\n",
      " [ 0.17424242]\n",
      " [ 0.11      ]\n",
      " [ 0.09583333]\n",
      " [ 0.16805556]\n",
      " [ 0.06041667]\n",
      " [ 0.19410173]\n",
      " [ 0.        ]\n",
      " [ 0.28541667]\n",
      " [ 0.25      ]\n",
      " [ 0.16369048]\n",
      " [ 0.14166667]\n",
      " [ 0.21875   ]\n",
      " [ 0.2       ]\n",
      " [ 0.18050964]\n",
      " [ 0.29166667]\n",
      " [ 0.        ]\n",
      " [-0.02588745]\n",
      " [ 0.23224638]\n",
      " [ 0.        ]\n",
      " [ 0.125     ]\n",
      " [ 0.07935606]\n",
      " [ 0.15      ]\n",
      " [ 0.25      ]\n",
      " [ 0.025     ]\n",
      " [ 0.12      ]\n",
      " [-0.078125  ]\n",
      " [-0.071875  ]\n",
      " [-0.08518518]\n",
      " [-1.        ]\n",
      " [ 0.14895833]\n",
      " [ 0.01333333]\n",
      " [ 0.2125    ]\n",
      " [ 0.44      ]\n",
      " [ 0.0125    ]\n",
      " [ 0.06944444]\n",
      " [ 0.12857143]\n",
      " [ 0.04375   ]\n",
      " [ 0.23459208]\n",
      " [ 0.13928571]\n",
      " [ 0.6       ]\n",
      " [ 0.515     ]\n",
      " [-0.13645833]\n",
      " [ 0.25208333]\n",
      " [ 0.28928571]\n",
      " [ 0.13125   ]\n",
      " [ 0.10476191]\n",
      " [ 0.2       ]\n",
      " [ 0.22857143]\n",
      " [ 0.21808036]\n",
      " [ 0.5       ]\n",
      " [ 0.01882716]\n",
      " [ 0.40333333]\n",
      " [ 0.1       ]\n",
      " [ 0.45833333]\n",
      " [ 0.18095238]\n",
      " [ 0.04791667]\n",
      " [ 0.07058823]\n",
      " [ 0.06924242]\n",
      " [ 0.02222222]\n",
      " [ 0.03629704]\n",
      " [-0.15555556]\n",
      " [-0.05      ]\n",
      " [ 0.23579545]\n",
      " [-0.5       ]\n",
      " [ 0.00449495]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.13863636]\n",
      " [-0.034375  ]\n",
      " [ 0.        ]\n",
      " [-0.66666667]\n",
      " [ 0.14583333]\n",
      " [ 0.10871212]\n",
      " [ 0.15659341]\n",
      " [-0.02630386]\n",
      " [-0.00779221]\n",
      " [ 0.6       ]\n",
      " [-0.125     ]\n",
      " [ 0.13333333]\n",
      " [ 0.25      ]\n",
      " [ 0.06966667]\n",
      " [ 0.1       ]\n",
      " [ 0.0375    ]\n",
      " [ 0.00595238]\n",
      " [ 0.01871693]\n",
      " [ 0.10548611]\n",
      " [ 0.259375  ]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.11470058]\n",
      " [ 0.06471861]\n",
      " [ 0.1694697 ]\n",
      " [ 0.20558036]\n",
      " [ 0.52      ]\n",
      " [ 0.06595238]\n",
      " [ 0.075     ]\n",
      " [ 0.22857143]\n",
      " [-0.03333333]\n",
      " [ 0.06666667]\n",
      " [-0.01458333]\n",
      " [ 0.1527972 ]\n",
      " [ 0.11071429]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.05277778]\n",
      " [-0.046875  ]\n",
      " [-0.00333333]\n",
      " [ 0.18229167]\n",
      " [ 0.0476087 ]\n",
      " [ 0.28214286]\n",
      " [ 0.0875    ]\n",
      " [-0.07777778]\n",
      " [ 0.26944444]\n",
      " [ 0.01      ]\n",
      " [-0.04466667]\n",
      " [ 0.14404762]\n",
      " [ 0.45      ]\n",
      " [ 0.09761905]\n",
      " [ 0.175     ]\n",
      " [-0.03333333]\n",
      " [ 0.45      ]\n",
      " [ 0.06428571]\n",
      " [ 0.1       ]\n",
      " [ 0.20767196]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.26346154]\n",
      " [ 0.14      ]\n",
      " [ 0.45833333]\n",
      " [-0.125     ]\n",
      " [ 0.11583333]\n",
      " [ 0.00476431]\n",
      " [ 0.17777778]\n",
      " [ 0.15833333]\n",
      " [-0.14861111]\n",
      " [-0.16666667]\n",
      " [ 0.06944444]\n",
      " [ 0.20857143]\n",
      " [ 0.05404762]\n",
      " [-0.04378157]\n",
      " [ 0.15170068]\n",
      " [ 0.125     ]\n",
      " [ 0.08374242]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.25909091]\n",
      " [ 0.185     ]\n",
      " [ 0.03592558]\n",
      " [ 0.22777778]\n",
      " [ 0.11308081]\n",
      " [ 0.17291667]\n",
      " [-0.25      ]\n",
      " [-0.65      ]\n",
      " [-0.11777778]\n",
      " [ 0.02380952]\n",
      " [ 0.        ]\n",
      " [ 0.30833333]\n",
      " [-0.015     ]\n",
      " [ 0.09393939]\n",
      " [ 0.0625    ]\n",
      " [ 0.13809524]\n",
      " [ 0.15333333]\n",
      " [-0.04356061]\n",
      " [ 0.09402597]\n",
      " [ 0.2147549 ]\n",
      " [ 0.3       ]\n",
      " [ 0.20189349]\n",
      " [ 0.28333333]\n",
      " [-0.35      ]\n",
      " [-0.1075    ]\n",
      " [ 0.45      ]\n",
      " [ 0.10555556]\n",
      " [ 0.3       ]\n",
      " [ 0.07052083]\n",
      " [ 0.1225    ]\n",
      " [ 0.17083333]\n",
      " [ 0.2       ]\n",
      " [ 0.06428571]\n",
      " [ 0.02579365]\n",
      " [-1.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.28583333]\n",
      " [ 0.16176471]\n",
      " [ 0.23333333]\n",
      " [ 0.01369048]\n",
      " [-0.45      ]\n",
      " [ 0.27575758]\n",
      " [ 0.21666667]\n",
      " [ 0.01111111]\n",
      " [ 0.08416667]\n",
      " [-0.01333333]\n",
      " [ 0.19284512]\n",
      " [ 0.16190476]\n",
      " [ 0.01715007]\n",
      " [ 0.01875   ]\n",
      " [ 0.15      ]\n",
      " [ 0.        ]\n",
      " [ 0.04785902]\n",
      " [ 0.20166667]\n",
      " [ 0.17142857]\n",
      " [ 0.28958333]\n",
      " [-0.04523809]\n",
      " [-0.01464646]\n",
      " [ 0.0625    ]\n",
      " [ 0.02166667]\n",
      " [ 0.4       ]\n",
      " [-0.03916667]\n",
      " [ 0.5       ]\n",
      " [-0.55      ]\n",
      " [ 0.02060606]\n",
      " [ 0.10833333]\n",
      " [ 0.14526515]\n",
      " [ 0.275     ]\n",
      " [ 0.16666667]\n",
      " [ 0.55      ]\n",
      " [ 0.05552083]\n",
      " [-0.06372549]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.01277778]\n",
      " [ 0.28888889]\n",
      " [ 0.375     ]\n",
      " [-0.31833333]\n",
      " [ 0.46666667]]\n",
      "X_test:  [[3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]]\n",
      "y_test:  [[ 0.275     ]\n",
      " [ 0.00555556]\n",
      " [ 0.13076923]\n",
      " [ 0.10504329]\n",
      " [ 0.25694444]\n",
      " [ 0.09285714]\n",
      " [-0.04166667]\n",
      " [ 0.125     ]\n",
      " [ 0.09675698]\n",
      " [ 0.5       ]\n",
      " [-0.01186526]\n",
      " [ 0.26666667]\n",
      " [ 0.06435185]\n",
      " [ 0.19027778]\n",
      " [-0.4       ]\n",
      " [ 0.18450635]\n",
      " [ 0.15      ]\n",
      " [ 0.24404762]\n",
      " [ 0.34545455]\n",
      " [-0.025     ]\n",
      " [ 0.2125    ]\n",
      " [ 0.1030303 ]\n",
      " [ 0.44444444]\n",
      " [-0.55555556]\n",
      " [ 0.4       ]\n",
      " [ 0.2765873 ]\n",
      " [ 0.13026245]\n",
      " [ 0.02453704]\n",
      " [ 0.15948162]\n",
      " [ 0.33888889]\n",
      " [ 0.08458333]\n",
      " [ 0.20160256]\n",
      " [ 0.121875  ]\n",
      " [ 0.19820513]\n",
      " [ 0.06428571]\n",
      " [ 0.09419643]\n",
      " [ 0.19888889]\n",
      " [ 0.1537037 ]\n",
      " [ 0.15208333]\n",
      " [ 0.        ]\n",
      " [ 0.38333333]\n",
      " [-0.2       ]\n",
      " [ 0.28571429]\n",
      " [-0.225     ]\n",
      " [ 0.33333333]\n",
      " [-0.29166667]\n",
      " [ 0.05769231]\n",
      " [ 0.01944444]\n",
      " [ 0.675     ]\n",
      " [ 0.09545455]\n",
      " [-0.17638889]\n",
      " [ 0.46666667]\n",
      " [ 0.07967607]\n",
      " [-0.2       ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.06527778]\n",
      " [ 0.21645833]\n",
      " [ 0.2575    ]\n",
      " [ 0.12380952]\n",
      " [ 0.01636905]\n",
      " [ 0.        ]\n",
      " [ 0.18333333]\n",
      " [ 0.25      ]\n",
      " [ 0.1625    ]\n",
      " [ 0.56666667]\n",
      " [ 0.155     ]\n",
      " [ 0.44      ]\n",
      " [ 0.3       ]\n",
      " [-0.01214286]\n",
      " [ 0.07142857]\n",
      " [-0.3625    ]\n",
      " [ 0.0625    ]\n",
      " [-0.31666667]\n",
      " [ 0.47916667]\n",
      " [ 0.06666667]\n",
      " [-0.20833333]\n",
      " [ 0.        ]\n",
      " [ 0.17381245]\n",
      " [ 0.07795056]\n",
      " [-0.14583333]\n",
      " [ 0.03125   ]\n",
      " [ 0.04494949]\n",
      " [ 0.11909091]\n",
      " [ 0.07944444]\n",
      " [-0.0625    ]\n",
      " [-0.1       ]\n",
      " [ 0.01136364]\n",
      " [ 0.10277778]\n",
      " [ 0.02777778]\n",
      " [ 0.        ]\n",
      " [ 0.08863636]]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1,1)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "X_test = X_test.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print (\"X_train: \", X_train)\n",
    "print (\"y_train: \", y_train)\n",
    "print (\"X_test: \", X_test)\n",
    "print (\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0408 - val_loss: 0.0431\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0400 - val_loss: 0.0429\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13ff0a4c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=30, epochs=20, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0428\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-0dcca7972e3a>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7, 7, 3, 7, 3, 7, 3, 7, 7, 3, 7, 7, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7,\n",
       "       3, 3, 3, 3, 3, 7, 3, 7, 7, 3, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3,\n",
       "       3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 3,\n",
       "       3, 7, 3, 7, 7, 3, 7, 3, 3, 7, 7, 7, 3, 3, 3, 3, 7, 3, 3, 3, 7, 7,\n",
       "       7, 3, 7, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test, batch_size=30)\n",
    "model.predict_classes(X_test, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 7, 1, 7, 1, 6, 7, 1, 7, 2, 7, 1, 7, 6, 7, 7, 7, 7, 6, 7, 1,\n",
       "       7, 6, 7, 7, 7, 2, 7, 7, 1, 7, 7, 7, 1, 1, 7, 7, 7, 2, 7, 6, 7, 6,\n",
       "       7, 6, 1, 2, 7, 1, 6, 7, 1, 6, 2, 2, 6, 7, 7, 7, 2, 2, 7, 7, 7, 7,\n",
       "       7, 7, 7, 2, 1, 6, 1, 6, 7, 1, 6, 2, 7, 1, 6, 2, 2, 1, 1, 6, 6, 2,\n",
       "       1, 2, 2, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(y_test, batch_size=30)\n",
    "model.predict_classes(y_test, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n"
     ]
    }
   ],
   "source": [
    "#Predict values test objects\n",
    "X_new = X_test\n",
    "y_proba = model.predict(X_new).round(2) # do I need the .round(2)?\n",
    "print(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  [[4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [6.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]]\n",
      "y_train:  [ 0.05        0.15918367  0.          0.16241497  0.          0.15642857\n",
      " -0.09270833 -0.06452991  0.01012397  0.43333333 -0.02916667  0.109375\n",
      "  0.05333333  0.1        -0.01458333  0.0795068   0.09        0.17857143\n",
      "  0.275       0.06009091  0.2625     -0.01916667  0.09166667  0.27559524\n",
      "  0.13318182  0.5         0.19622024  0.          0.2         0.21785714\n",
      "  0.25       -0.10833333  0.15625     0.07666667  0.2625      0.22222222\n",
      "  0.0625     -0.08636364  0.18083333  0.          0.09970238  0.22166667\n",
      "  0.095       0.08848485  0.09333333  0.07238095 -0.175       0.03541667\n",
      "  0.17424242  0.11        0.09583333  0.16805556  0.06041667  0.19410173\n",
      "  0.          0.28541667  0.25        0.16369048  0.14166667  0.21875\n",
      "  0.2         0.18050964  0.29166667  0.         -0.02588745  0.23224638\n",
      "  0.          0.125       0.07935606  0.15        0.25        0.025\n",
      "  0.12       -0.078125   -0.071875   -0.08518518 -1.          0.14895833\n",
      "  0.01333333  0.2125      0.44        0.0125      0.06944444  0.12857143\n",
      "  0.04375     0.23459208  0.13928571  0.6         0.515      -0.13645833\n",
      "  0.25208333  0.28928571  0.13125     0.10476191  0.2         0.22857143\n",
      "  0.21808036  0.5         0.01882716  0.40333333  0.1         0.45833333\n",
      "  0.18095238  0.04791667  0.07058823  0.06924242  0.02222222  0.03629704\n",
      " -0.15555556 -0.05        0.23579545 -0.5         0.00449495  0.16666667\n",
      "  0.          0.13863636 -0.034375    0.         -0.66666667  0.14583333\n",
      "  0.10871212  0.15659341 -0.02630386 -0.00779221  0.6        -0.125\n",
      "  0.13333333  0.25        0.06966667  0.1         0.0375      0.00595238\n",
      "  0.01871693  0.10548611  0.259375    0.          0.2         0.11470058\n",
      "  0.06471861  0.1694697   0.20558036  0.52        0.06595238  0.075\n",
      "  0.22857143 -0.03333333  0.06666667 -0.01458333  0.1527972   0.11071429\n",
      "  0.          0.          0.05277778 -0.046875   -0.00333333  0.18229167\n",
      "  0.0476087   0.28214286  0.0875     -0.07777778  0.26944444  0.01\n",
      " -0.04466667  0.14404762  0.45        0.09761905  0.175      -0.03333333\n",
      "  0.45        0.06428571  0.1         0.20767196  0.          0.\n",
      "  0.26346154  0.14        0.45833333 -0.125       0.11583333  0.00476431\n",
      "  0.17777778  0.15833333 -0.14861111 -0.16666667  0.06944444  0.20857143\n",
      "  0.05404762 -0.04378157  0.15170068  0.125       0.08374242  0.16666667\n",
      "  0.          0.25        0.25909091  0.185       0.03592558  0.22777778\n",
      "  0.11308081  0.17291667 -0.25       -0.65       -0.11777778  0.02380952\n",
      "  0.          0.30833333 -0.015       0.09393939  0.0625      0.13809524\n",
      "  0.15333333 -0.04356061  0.09402597  0.2147549   0.3         0.20189349\n",
      "  0.28333333 -0.35       -0.1075      0.45        0.10555556  0.3\n",
      "  0.07052083  0.1225      0.17083333  0.2         0.06428571  0.02579365\n",
      " -1.          0.25        0.28583333  0.16176471  0.23333333  0.01369048\n",
      " -0.45        0.27575758  0.21666667  0.01111111  0.08416667 -0.01333333\n",
      "  0.19284512  0.16190476  0.01715007  0.01875     0.15        0.\n",
      "  0.04785902  0.20166667  0.17142857  0.28958333 -0.04523809 -0.01464646\n",
      "  0.0625      0.02166667  0.4        -0.03916667  0.5        -0.55\n",
      "  0.02060606  0.10833333  0.14526515  0.275       0.16666667  0.55\n",
      "  0.05552083 -0.06372549  0.          0.         -0.01277778  0.28888889\n",
      "  0.375      -0.31833333  0.46666667]\n",
      "X_test:  [[3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]]\n",
      "y_test:  [ 0.275       0.00555556  0.13076923  0.10504329  0.25694444  0.09285714\n",
      " -0.04166667  0.125       0.09675698  0.5        -0.01186526  0.26666667\n",
      "  0.06435185  0.19027778 -0.4         0.18450635  0.15        0.24404762\n",
      "  0.34545455 -0.025       0.2125      0.1030303   0.44444444 -0.55555556\n",
      "  0.4         0.2765873   0.13026245  0.02453704  0.15948162  0.33888889\n",
      "  0.08458333  0.20160256  0.121875    0.19820513  0.06428571  0.09419643\n",
      "  0.19888889  0.1537037   0.15208333  0.          0.38333333 -0.2\n",
      "  0.28571429 -0.225       0.33333333 -0.29166667  0.05769231  0.01944444\n",
      "  0.675       0.09545455 -0.17638889  0.46666667  0.07967607 -0.2\n",
      "  0.          0.         -0.06527778  0.21645833  0.2575      0.12380952\n",
      "  0.01636905  0.          0.18333333  0.25        0.1625      0.56666667\n",
      "  0.155       0.44        0.3        -0.01214286  0.07142857 -0.3625\n",
      "  0.0625     -0.31666667  0.47916667  0.06666667 -0.20833333  0.\n",
      "  0.17381245  0.07795056 -0.14583333  0.03125     0.04494949  0.11909091\n",
      "  0.07944444 -0.0625     -0.1         0.01136364  0.10277778  0.02777778\n",
      "  0.          0.08863636]\n"
     ]
    }
   ],
   "source": [
    "# now let's try a prediction based on Q40\n",
    "\n",
    "X_train2, X_test2, y_train, y_test = model_selection.train_test_split(NLPdata[:,1], NLPdata[:,2], train_size=0.75,test_size=0.25, random_state=101)\n",
    "print (\"X_train: \", X_train)\n",
    "print (\"y_train: \", y_train)\n",
    "print (\"X_test: \", X_test)\n",
    "print (\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train2 = X_train2.reshape(-1,1)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "X_test2 = X_test2.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print (\"X_train2: \", X_train2)\n",
    "print (\"y_train: \", y_train)\n",
    "print (\"X_test2: \", X_test2)\n",
    "print (\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0398 - val_loss: 0.0428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x140650c10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train2, y_train, batch_size=30, epochs=20, verbose=1, validation_data=(X_test2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0428\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test2, y_test, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 1, 5, 1, 4, 1, 4, 5, 1, 1, 4, 4, 1, 1, 4, 4, 1, 1, 4, 5, 4,\n",
       "       1, 1, 4, 4, 1, 5, 4, 1, 4, 5, 1, 1, 1, 1, 1, 1, 4, 5, 4, 1, 4, 4,\n",
       "       1, 5, 4, 1, 5, 1, 1, 1, 1, 4, 4, 1, 5, 4, 4, 4, 1, 4, 5, 5, 1, 4,\n",
       "       4, 1, 1, 4, 5, 4, 4, 4, 4, 1, 1, 4, 4, 5, 1, 5, 4, 4, 4, 1, 1, 1,\n",
       "       1, 4, 4, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test2, batch_size=30)\n",
    "model.predict_classes(X_test2, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 1, 8, 1, 8, 5, 1, 8, 1, 2, 1, 8, 1, 6, 1, 1, 1, 1, 5, 1, 8,\n",
       "       1, 6, 1, 1, 1, 5, 1, 1, 8, 1, 1, 1, 8, 8, 1, 1, 1, 4, 1, 6, 1, 6,\n",
       "       1, 6, 5, 8, 1, 8, 6, 1, 8, 6, 4, 4, 5, 1, 1, 1, 8, 4, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 8, 6, 8, 6, 1, 8, 6, 4, 1, 8, 6, 5, 5, 1, 8, 5, 5, 8,\n",
       "       8, 5, 4, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(y_test, batch_size=30)\n",
    "model.predict_classes(y_test, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09994822 0.09962972 0.10006044 0.10015477 0.09997173 0.10018791\n",
      "  0.10014421 0.09986448 0.09993964 0.10009894]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994822 0.09962972 0.10006044 0.10015477 0.09997173 0.10018791\n",
      "  0.10014421 0.09986448 0.09993964 0.10009894]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.1000222  0.1002086  0.10010935 0.09980931 0.09971716 0.10010974\n",
      "  0.10011105 0.10003578 0.09977765 0.10009924]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.1000222  0.1002086  0.10010935 0.09980931 0.09971716 0.10010974\n",
      "  0.10011105 0.10003578 0.09977765 0.10009924]\n",
      " [0.1000222  0.1002086  0.10010935 0.09980931 0.09971716 0.10010974\n",
      "  0.10011105 0.10003578 0.09977765 0.10009924]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994822 0.09962972 0.10006044 0.10015477 0.09997173 0.10018791\n",
      "  0.10014421 0.09986448 0.09993964 0.10009894]\n",
      " [0.10009629 0.09952457 0.09985424 0.10034314 0.10046484 0.09988546\n",
      "  0.0998387  0.09974682 0.10027613 0.09996981]\n",
      " [0.1000222  0.1002086  0.10010935 0.09980931 0.09971716 0.10010974\n",
      "  0.10011105 0.10003578 0.09977765 0.10009924]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.10009629 0.09952457 0.09985424 0.10034314 0.10046484 0.09988546\n",
      "  0.0998387  0.09974682 0.10027613 0.09996981]\n",
      " [0.10009629 0.09952457 0.09985424 0.10034314 0.10046484 0.09988546\n",
      "  0.0998387  0.09974682 0.10027613 0.09996981]\n",
      " [0.1000222  0.1002086  0.10010935 0.09980931 0.09971716 0.10010974\n",
      "  0.10011105 0.10003578 0.09977765 0.10009924]\n",
      " [0.09994822 0.09962972 0.10006044 0.10015477 0.09997173 0.10018791\n",
      "  0.10014421 0.09986448 0.09993964 0.10009894]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09998539 0.10000912 0.10010646 0.09991924 0.09977337 0.10015895\n",
      "  0.10014445 0.09998294 0.09981182 0.10010825]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.1000222  0.1002086  0.10010935 0.09980931 0.09971716 0.10010974\n",
      "  0.10011105 0.10003578 0.09977765 0.10009924]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994822 0.09962972 0.10006044 0.10015477 0.09997173 0.10018791\n",
      "  0.10014421 0.09986448 0.09993964 0.10009894]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09994822 0.09962972 0.10006044 0.10015477 0.09997173 0.10018791\n",
      "  0.10014421 0.09986448 0.09993964 0.10009894]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09998539 0.10000912 0.10010646 0.09991924 0.09977337 0.10015895\n",
      "  0.10014444 0.09998294 0.09981181 0.10010826]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.1000222  0.1002086  0.10010935 0.09980931 0.09971716 0.10010974\n",
      "  0.10011105 0.10003578 0.09977765 0.10009924]\n",
      " [0.1000222  0.1002086  0.10010935 0.09980931 0.09971716 0.10010974\n",
      "  0.10011105 0.10003578 0.09977765 0.10009924]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.1000222  0.1002086  0.10010935 0.09980931 0.09971716 0.10010974\n",
      "  0.10011105 0.10003578 0.09977765 0.10009924]\n",
      " [0.09994822 0.09962972 0.10006044 0.10015477 0.09997173 0.10018791\n",
      "  0.10014421 0.09986448 0.09993964 0.10009894]\n",
      " [0.10009629 0.09952457 0.09985424 0.10034314 0.10046484 0.09988546\n",
      "  0.0998387  0.09974682 0.10027613 0.09996981]\n",
      " [0.10009629 0.09952457 0.09985424 0.10034314 0.10046484 0.09988546\n",
      "  0.0998387  0.09974682 0.10027613 0.09996981]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994822 0.09962972 0.10006045 0.10015477 0.09997173 0.10018791\n",
      "  0.10014421 0.09986447 0.09993964 0.10009894]\n",
      " [0.09994822 0.09962972 0.10006045 0.10015477 0.09997173 0.10018791\n",
      "  0.10014421 0.09986447 0.09993964 0.10009894]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.1000222  0.1002086  0.10010935 0.09980931 0.09971716 0.10010974\n",
      "  0.10011105 0.10003578 0.09977765 0.10009924]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994822 0.09962972 0.10006044 0.10015477 0.09997173 0.10018791\n",
      "  0.10014421 0.09986448 0.09993964 0.10009894]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.10009629 0.09952457 0.09985424 0.10034314 0.10046484 0.09988546\n",
      "  0.0998387  0.09974682 0.10027613 0.09996981]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.1000222  0.1002086  0.10010935 0.09980931 0.09971716 0.10010974\n",
      "  0.10011105 0.10003578 0.09977765 0.10009924]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994822 0.09962972 0.10006044 0.10015477 0.09997173 0.10018791\n",
      "  0.10014421 0.09986448 0.09993964 0.10009894]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09994856 0.09980997 0.1001035  0.10002925 0.09982955 0.10020814\n",
      "  0.10017777 0.09993008 0.09984594 0.1001172 ]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.1000222  0.1002086  0.10010935 0.09980931 0.09971716 0.10010974\n",
      "  0.10011105 0.10003578 0.09977765 0.10009924]\n",
      " [0.09997921 0.10032825 0.10000892 0.0997367  0.09975554 0.10000326\n",
      "  0.10008276 0.10019363 0.09991469 0.09999702]\n",
      " [0.09994226 0.09995889 0.10003504 0.10007003 0.10007544 0.09998531\n",
      "  0.09989804 0.10002753 0.10005434 0.09995313]\n",
      " [0.10009629 0.09952457 0.09985424 0.10034314 0.10046484 0.09988546\n",
      "  0.0998387  0.09974682 0.10027613 0.09996981]\n",
      " [0.09994822 0.09962972 0.10006044 0.10015477 0.09997173 0.10018791\n",
      "  0.10014421 0.09986448 0.09993964 0.10009894]]\n"
     ]
    }
   ],
   "source": [
    "#Predict values test objects\n",
    "X_new = X_test2\n",
    "y_proba = model.predict(X_new) # do I need the .round(2)?\n",
    "print(y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing round 2,  12/28/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLPdata = pd.read_csv('nlpdata.csv')\n",
    "NLPdata = NLPdata.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_dim=1)) \n",
    "#model.add(Dense(28, activation='relu')) \n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  [4. 4. 5. 4. 4. 4. 4. 4. 3. 1. 4. 4. 3. 4. 4. 4. 4. 4. 4. 2. 4. 4. 4. 3.\n",
      " 1. 2. 3. 1. 4. 4. 6. 4. 4. 1. 4. 4. 1. 4. 4. 2. 3. 4. 4. 2. 1. 5. 3. 2.\n",
      " 5. 4. 4. 2. 4. 4. 1. 1. 4. 4. 2. 4. 2. 3. 2. 4. 4. 4. 4. 4. 4. 3. 6. 3.\n",
      " 3. 4. 3. 4. 2. 5. 2. 1. 4. 1. 4. 3. 4. 1. 4. 1. 3. 1. 3. 3. 3. 4. 4. 5.\n",
      " 4. 4. 3. 4. 4. 4. 4. 4. 2. 2. 4. 4. 4. 4. 4. 1. 2. 3. 2. 4. 4. 4. 1. 4.\n",
      " 5. 1. 5. 4. 4. 2. 4. 3. 2. 4. 3. 3. 4. 1. 3. 5. 3. 2. 3. 5. 4. 4. 4. 4.\n",
      " 3. 1. 4. 3. 2. 2. 4. 2. 4. 4. 4. 4. 2. 4. 4. 3. 4. 1. 4. 4. 4. 3. 4. 4.\n",
      " 2. 2. 2. 4. 4. 3. 4. 3. 4. 4. 2. 2. 4. 4. 5. 1. 2. 4. 4. 2. 2. 4. 3. 4.\n",
      " 4. 4. 5. 4. 2. 3. 1. 4. 3. 4. 4. 3. 2. 4. 1. 4. 4. 2. 4. 4. 4. 4. 4. 4.\n",
      " 2. 2. 4. 2. 4. 4. 4. 5. 4. 3. 4. 4. 3. 2. 1. 3. 4. 4. 3. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 1. 4. 4. 3. 1. 3. 4. 4. 4. 4. 4. 4. 2. 4. 2. 3.\n",
      " 3. 4. 2. 4. 4. 4. 4. 4. 1.]\n",
      "y_train:  [ 0.05        0.15918367  0.          0.16241497  0.          0.15642857\n",
      " -0.09270833 -0.06452991  0.01012397  0.43333333 -0.02916667  0.109375\n",
      "  0.05333333  0.1        -0.01458333  0.0795068   0.09        0.17857143\n",
      "  0.275       0.06009091  0.2625     -0.01916667  0.09166667  0.27559524\n",
      "  0.13318182  0.5         0.19622024  0.          0.2         0.21785714\n",
      "  0.25       -0.10833333  0.15625     0.07666667  0.2625      0.22222222\n",
      "  0.0625     -0.08636364  0.18083333  0.          0.09970238  0.22166667\n",
      "  0.095       0.08848485  0.09333333  0.07238095 -0.175       0.03541667\n",
      "  0.17424242  0.11        0.09583333  0.16805556  0.06041667  0.19410173\n",
      "  0.          0.28541667  0.25        0.16369048  0.14166667  0.21875\n",
      "  0.2         0.18050964  0.29166667  0.         -0.02588745  0.23224638\n",
      "  0.          0.125       0.07935606  0.15        0.25        0.025\n",
      "  0.12       -0.078125   -0.071875   -0.08518518 -1.          0.14895833\n",
      "  0.01333333  0.2125      0.44        0.0125      0.06944444  0.12857143\n",
      "  0.04375     0.23459208  0.13928571  0.6         0.515      -0.13645833\n",
      "  0.25208333  0.28928571  0.13125     0.10476191  0.2         0.22857143\n",
      "  0.21808036  0.5         0.01882716  0.40333333  0.1         0.45833333\n",
      "  0.18095238  0.04791667  0.07058823  0.06924242  0.02222222  0.03629704\n",
      " -0.15555556 -0.05        0.23579545 -0.5         0.00449495  0.16666667\n",
      "  0.          0.13863636 -0.034375    0.         -0.66666667  0.14583333\n",
      "  0.10871212  0.15659341 -0.02630386 -0.00779221  0.6        -0.125\n",
      "  0.13333333  0.25        0.06966667  0.1         0.0375      0.00595238\n",
      "  0.01871693  0.10548611  0.259375    0.          0.2         0.11470058\n",
      "  0.06471861  0.1694697   0.20558036  0.52        0.06595238  0.075\n",
      "  0.22857143 -0.03333333  0.06666667 -0.01458333  0.1527972   0.11071429\n",
      "  0.          0.          0.05277778 -0.046875   -0.00333333  0.18229167\n",
      "  0.0476087   0.28214286  0.0875     -0.07777778  0.26944444  0.01\n",
      " -0.04466667  0.14404762  0.45        0.09761905  0.175      -0.03333333\n",
      "  0.45        0.06428571  0.1         0.20767196  0.          0.\n",
      "  0.26346154  0.14        0.45833333 -0.125       0.11583333  0.00476431\n",
      "  0.17777778  0.15833333 -0.14861111 -0.16666667  0.06944444  0.20857143\n",
      "  0.05404762 -0.04378157  0.15170068  0.125       0.08374242  0.16666667\n",
      "  0.          0.25        0.25909091  0.185       0.03592558  0.22777778\n",
      "  0.11308081  0.17291667 -0.25       -0.65       -0.11777778  0.02380952\n",
      "  0.          0.30833333 -0.015       0.09393939  0.0625      0.13809524\n",
      "  0.15333333 -0.04356061  0.09402597  0.2147549   0.3         0.20189349\n",
      "  0.28333333 -0.35       -0.1075      0.45        0.10555556  0.3\n",
      "  0.07052083  0.1225      0.17083333  0.2         0.06428571  0.02579365\n",
      " -1.          0.25        0.28583333  0.16176471  0.23333333  0.01369048\n",
      " -0.45        0.27575758  0.21666667  0.01111111  0.08416667 -0.01333333\n",
      "  0.19284512  0.16190476  0.01715007  0.01875     0.15        0.\n",
      "  0.04785902  0.20166667  0.17142857  0.28958333 -0.04523809 -0.01464646\n",
      "  0.0625      0.02166667  0.4        -0.03916667  0.5        -0.55\n",
      "  0.02060606  0.10833333  0.14526515  0.275       0.16666667  0.55\n",
      "  0.05552083 -0.06372549  0.          0.         -0.01277778  0.28888889\n",
      "  0.375      -0.31833333  0.46666667]\n",
      "X_test:  [3. 3. 4. 2. 4. 3. 4. 2. 3. 4. 3. 3. 5. 4. 4. 4. 4. 4. 4. 2. 1. 2. 4. 4.\n",
      " 4. 4. 4. 1. 4. 2. 2. 4. 4. 3. 1. 2. 2. 1. 2. 2. 1. 2. 3. 4. 4. 4. 4. 4.\n",
      " 4. 2. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 3. 3. 1. 3. 4. 4. 2. 4. 3. 2. 4.\n",
      " 3. 5. 4. 2. 1. 3. 4. 4. 4. 4. 3. 4. 4. 5. 3. 1. 2. 4. 1. 4.]\n",
      "y_test:  [ 0.275       0.00555556  0.13076923  0.10504329  0.25694444  0.09285714\n",
      " -0.04166667  0.125       0.09675698  0.5        -0.01186526  0.26666667\n",
      "  0.06435185  0.19027778 -0.4         0.18450635  0.15        0.24404762\n",
      "  0.34545455 -0.025       0.2125      0.1030303   0.44444444 -0.55555556\n",
      "  0.4         0.2765873   0.13026245  0.02453704  0.15948162  0.33888889\n",
      "  0.08458333  0.20160256  0.121875    0.19820513  0.06428571  0.09419643\n",
      "  0.19888889  0.1537037   0.15208333  0.          0.38333333 -0.2\n",
      "  0.28571429 -0.225       0.33333333 -0.29166667  0.05769231  0.01944444\n",
      "  0.675       0.09545455 -0.17638889  0.46666667  0.07967607 -0.2\n",
      "  0.          0.         -0.06527778  0.21645833  0.2575      0.12380952\n",
      "  0.01636905  0.          0.18333333  0.25        0.1625      0.56666667\n",
      "  0.155       0.44        0.3        -0.01214286  0.07142857 -0.3625\n",
      "  0.0625     -0.31666667  0.47916667  0.06666667 -0.20833333  0.\n",
      "  0.17381245  0.07795056 -0.14583333  0.03125     0.04494949  0.11909091\n",
      "  0.07944444 -0.0625     -0.1         0.01136364  0.10277778  0.02777778\n",
      "  0.          0.08863636]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection as model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(NLPdata[:,0], NLPdata[:,2], train_size=0.75,test_size=0.25, random_state=101)\n",
    "print (\"X_train: \", X_train)\n",
    "print (\"y_train: \", y_train)\n",
    "print (\"X_test: \", X_test)\n",
    "print (\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  [[4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [6.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]]\n",
      "y_train:  [[ 0.05      ]\n",
      " [ 0.15918367]\n",
      " [ 0.        ]\n",
      " [ 0.16241497]\n",
      " [ 0.        ]\n",
      " [ 0.15642857]\n",
      " [-0.09270833]\n",
      " [-0.06452991]\n",
      " [ 0.01012397]\n",
      " [ 0.43333333]\n",
      " [-0.02916667]\n",
      " [ 0.109375  ]\n",
      " [ 0.05333333]\n",
      " [ 0.1       ]\n",
      " [-0.01458333]\n",
      " [ 0.0795068 ]\n",
      " [ 0.09      ]\n",
      " [ 0.17857143]\n",
      " [ 0.275     ]\n",
      " [ 0.06009091]\n",
      " [ 0.2625    ]\n",
      " [-0.01916667]\n",
      " [ 0.09166667]\n",
      " [ 0.27559524]\n",
      " [ 0.13318182]\n",
      " [ 0.5       ]\n",
      " [ 0.19622024]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.21785714]\n",
      " [ 0.25      ]\n",
      " [-0.10833333]\n",
      " [ 0.15625   ]\n",
      " [ 0.07666667]\n",
      " [ 0.2625    ]\n",
      " [ 0.22222222]\n",
      " [ 0.0625    ]\n",
      " [-0.08636364]\n",
      " [ 0.18083333]\n",
      " [ 0.        ]\n",
      " [ 0.09970238]\n",
      " [ 0.22166667]\n",
      " [ 0.095     ]\n",
      " [ 0.08848485]\n",
      " [ 0.09333333]\n",
      " [ 0.07238095]\n",
      " [-0.175     ]\n",
      " [ 0.03541667]\n",
      " [ 0.17424242]\n",
      " [ 0.11      ]\n",
      " [ 0.09583333]\n",
      " [ 0.16805556]\n",
      " [ 0.06041667]\n",
      " [ 0.19410173]\n",
      " [ 0.        ]\n",
      " [ 0.28541667]\n",
      " [ 0.25      ]\n",
      " [ 0.16369048]\n",
      " [ 0.14166667]\n",
      " [ 0.21875   ]\n",
      " [ 0.2       ]\n",
      " [ 0.18050964]\n",
      " [ 0.29166667]\n",
      " [ 0.        ]\n",
      " [-0.02588745]\n",
      " [ 0.23224638]\n",
      " [ 0.        ]\n",
      " [ 0.125     ]\n",
      " [ 0.07935606]\n",
      " [ 0.15      ]\n",
      " [ 0.25      ]\n",
      " [ 0.025     ]\n",
      " [ 0.12      ]\n",
      " [-0.078125  ]\n",
      " [-0.071875  ]\n",
      " [-0.08518518]\n",
      " [-1.        ]\n",
      " [ 0.14895833]\n",
      " [ 0.01333333]\n",
      " [ 0.2125    ]\n",
      " [ 0.44      ]\n",
      " [ 0.0125    ]\n",
      " [ 0.06944444]\n",
      " [ 0.12857143]\n",
      " [ 0.04375   ]\n",
      " [ 0.23459208]\n",
      " [ 0.13928571]\n",
      " [ 0.6       ]\n",
      " [ 0.515     ]\n",
      " [-0.13645833]\n",
      " [ 0.25208333]\n",
      " [ 0.28928571]\n",
      " [ 0.13125   ]\n",
      " [ 0.10476191]\n",
      " [ 0.2       ]\n",
      " [ 0.22857143]\n",
      " [ 0.21808036]\n",
      " [ 0.5       ]\n",
      " [ 0.01882716]\n",
      " [ 0.40333333]\n",
      " [ 0.1       ]\n",
      " [ 0.45833333]\n",
      " [ 0.18095238]\n",
      " [ 0.04791667]\n",
      " [ 0.07058823]\n",
      " [ 0.06924242]\n",
      " [ 0.02222222]\n",
      " [ 0.03629704]\n",
      " [-0.15555556]\n",
      " [-0.05      ]\n",
      " [ 0.23579545]\n",
      " [-0.5       ]\n",
      " [ 0.00449495]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.13863636]\n",
      " [-0.034375  ]\n",
      " [ 0.        ]\n",
      " [-0.66666667]\n",
      " [ 0.14583333]\n",
      " [ 0.10871212]\n",
      " [ 0.15659341]\n",
      " [-0.02630386]\n",
      " [-0.00779221]\n",
      " [ 0.6       ]\n",
      " [-0.125     ]\n",
      " [ 0.13333333]\n",
      " [ 0.25      ]\n",
      " [ 0.06966667]\n",
      " [ 0.1       ]\n",
      " [ 0.0375    ]\n",
      " [ 0.00595238]\n",
      " [ 0.01871693]\n",
      " [ 0.10548611]\n",
      " [ 0.259375  ]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.11470058]\n",
      " [ 0.06471861]\n",
      " [ 0.1694697 ]\n",
      " [ 0.20558036]\n",
      " [ 0.52      ]\n",
      " [ 0.06595238]\n",
      " [ 0.075     ]\n",
      " [ 0.22857143]\n",
      " [-0.03333333]\n",
      " [ 0.06666667]\n",
      " [-0.01458333]\n",
      " [ 0.1527972 ]\n",
      " [ 0.11071429]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.05277778]\n",
      " [-0.046875  ]\n",
      " [-0.00333333]\n",
      " [ 0.18229167]\n",
      " [ 0.0476087 ]\n",
      " [ 0.28214286]\n",
      " [ 0.0875    ]\n",
      " [-0.07777778]\n",
      " [ 0.26944444]\n",
      " [ 0.01      ]\n",
      " [-0.04466667]\n",
      " [ 0.14404762]\n",
      " [ 0.45      ]\n",
      " [ 0.09761905]\n",
      " [ 0.175     ]\n",
      " [-0.03333333]\n",
      " [ 0.45      ]\n",
      " [ 0.06428571]\n",
      " [ 0.1       ]\n",
      " [ 0.20767196]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.26346154]\n",
      " [ 0.14      ]\n",
      " [ 0.45833333]\n",
      " [-0.125     ]\n",
      " [ 0.11583333]\n",
      " [ 0.00476431]\n",
      " [ 0.17777778]\n",
      " [ 0.15833333]\n",
      " [-0.14861111]\n",
      " [-0.16666667]\n",
      " [ 0.06944444]\n",
      " [ 0.20857143]\n",
      " [ 0.05404762]\n",
      " [-0.04378157]\n",
      " [ 0.15170068]\n",
      " [ 0.125     ]\n",
      " [ 0.08374242]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.25909091]\n",
      " [ 0.185     ]\n",
      " [ 0.03592558]\n",
      " [ 0.22777778]\n",
      " [ 0.11308081]\n",
      " [ 0.17291667]\n",
      " [-0.25      ]\n",
      " [-0.65      ]\n",
      " [-0.11777778]\n",
      " [ 0.02380952]\n",
      " [ 0.        ]\n",
      " [ 0.30833333]\n",
      " [-0.015     ]\n",
      " [ 0.09393939]\n",
      " [ 0.0625    ]\n",
      " [ 0.13809524]\n",
      " [ 0.15333333]\n",
      " [-0.04356061]\n",
      " [ 0.09402597]\n",
      " [ 0.2147549 ]\n",
      " [ 0.3       ]\n",
      " [ 0.20189349]\n",
      " [ 0.28333333]\n",
      " [-0.35      ]\n",
      " [-0.1075    ]\n",
      " [ 0.45      ]\n",
      " [ 0.10555556]\n",
      " [ 0.3       ]\n",
      " [ 0.07052083]\n",
      " [ 0.1225    ]\n",
      " [ 0.17083333]\n",
      " [ 0.2       ]\n",
      " [ 0.06428571]\n",
      " [ 0.02579365]\n",
      " [-1.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.28583333]\n",
      " [ 0.16176471]\n",
      " [ 0.23333333]\n",
      " [ 0.01369048]\n",
      " [-0.45      ]\n",
      " [ 0.27575758]\n",
      " [ 0.21666667]\n",
      " [ 0.01111111]\n",
      " [ 0.08416667]\n",
      " [-0.01333333]\n",
      " [ 0.19284512]\n",
      " [ 0.16190476]\n",
      " [ 0.01715007]\n",
      " [ 0.01875   ]\n",
      " [ 0.15      ]\n",
      " [ 0.        ]\n",
      " [ 0.04785902]\n",
      " [ 0.20166667]\n",
      " [ 0.17142857]\n",
      " [ 0.28958333]\n",
      " [-0.04523809]\n",
      " [-0.01464646]\n",
      " [ 0.0625    ]\n",
      " [ 0.02166667]\n",
      " [ 0.4       ]\n",
      " [-0.03916667]\n",
      " [ 0.5       ]\n",
      " [-0.55      ]\n",
      " [ 0.02060606]\n",
      " [ 0.10833333]\n",
      " [ 0.14526515]\n",
      " [ 0.275     ]\n",
      " [ 0.16666667]\n",
      " [ 0.55      ]\n",
      " [ 0.05552083]\n",
      " [-0.06372549]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.01277778]\n",
      " [ 0.28888889]\n",
      " [ 0.375     ]\n",
      " [-0.31833333]\n",
      " [ 0.46666667]]\n",
      "X_test:  [[3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]]\n",
      "y_test:  [[ 0.275     ]\n",
      " [ 0.00555556]\n",
      " [ 0.13076923]\n",
      " [ 0.10504329]\n",
      " [ 0.25694444]\n",
      " [ 0.09285714]\n",
      " [-0.04166667]\n",
      " [ 0.125     ]\n",
      " [ 0.09675698]\n",
      " [ 0.5       ]\n",
      " [-0.01186526]\n",
      " [ 0.26666667]\n",
      " [ 0.06435185]\n",
      " [ 0.19027778]\n",
      " [-0.4       ]\n",
      " [ 0.18450635]\n",
      " [ 0.15      ]\n",
      " [ 0.24404762]\n",
      " [ 0.34545455]\n",
      " [-0.025     ]\n",
      " [ 0.2125    ]\n",
      " [ 0.1030303 ]\n",
      " [ 0.44444444]\n",
      " [-0.55555556]\n",
      " [ 0.4       ]\n",
      " [ 0.2765873 ]\n",
      " [ 0.13026245]\n",
      " [ 0.02453704]\n",
      " [ 0.15948162]\n",
      " [ 0.33888889]\n",
      " [ 0.08458333]\n",
      " [ 0.20160256]\n",
      " [ 0.121875  ]\n",
      " [ 0.19820513]\n",
      " [ 0.06428571]\n",
      " [ 0.09419643]\n",
      " [ 0.19888889]\n",
      " [ 0.1537037 ]\n",
      " [ 0.15208333]\n",
      " [ 0.        ]\n",
      " [ 0.38333333]\n",
      " [-0.2       ]\n",
      " [ 0.28571429]\n",
      " [-0.225     ]\n",
      " [ 0.33333333]\n",
      " [-0.29166667]\n",
      " [ 0.05769231]\n",
      " [ 0.01944444]\n",
      " [ 0.675     ]\n",
      " [ 0.09545455]\n",
      " [-0.17638889]\n",
      " [ 0.46666667]\n",
      " [ 0.07967607]\n",
      " [-0.2       ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.06527778]\n",
      " [ 0.21645833]\n",
      " [ 0.2575    ]\n",
      " [ 0.12380952]\n",
      " [ 0.01636905]\n",
      " [ 0.        ]\n",
      " [ 0.18333333]\n",
      " [ 0.25      ]\n",
      " [ 0.1625    ]\n",
      " [ 0.56666667]\n",
      " [ 0.155     ]\n",
      " [ 0.44      ]\n",
      " [ 0.3       ]\n",
      " [-0.01214286]\n",
      " [ 0.07142857]\n",
      " [-0.3625    ]\n",
      " [ 0.0625    ]\n",
      " [-0.31666667]\n",
      " [ 0.47916667]\n",
      " [ 0.06666667]\n",
      " [-0.20833333]\n",
      " [ 0.        ]\n",
      " [ 0.17381245]\n",
      " [ 0.07795056]\n",
      " [-0.14583333]\n",
      " [ 0.03125   ]\n",
      " [ 0.04494949]\n",
      " [ 0.11909091]\n",
      " [ 0.07944444]\n",
      " [-0.0625    ]\n",
      " [-0.1       ]\n",
      " [ 0.01136364]\n",
      " [ 0.10277778]\n",
      " [ 0.02777778]\n",
      " [ 0.        ]\n",
      " [ 0.08863636]]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1,1)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "X_test = X_test.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print (\"X_train: \", X_train)\n",
    "print (\"y_train: \", y_train)\n",
    "print (\"X_test: \", X_test)\n",
    "print (\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0444 - val_loss: 0.0450\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.0437\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.0432\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.0430\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.0429\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.0428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1406a8400>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=10, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04280076175928116"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test) #, batch_size=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 7, 4, 7, 4, 7, 4, 4, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4,\n",
       "       7, 7, 7, 7, 7, 4, 7, 4, 4, 7, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7,\n",
       "       7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 7,\n",
       "       7, 4, 7, 4, 4, 7, 4, 7, 7, 4, 4, 4, 7, 7, 7, 7, 4, 7, 7, 7, 4, 4,\n",
       "       4, 7, 4, 7])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test, batch_size=30)\n",
    "model.predict_classes(X_test, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 9, 9, 9, 4, 9, 9, 9, 9, 4, 9, 4, 9, 9, 5, 9, 9, 4, 4, 9, 4, 9,\n",
       "       4, 5, 4, 4, 9, 9, 9, 4, 9, 4, 9, 9, 9, 9, 4, 9, 9, 9, 4, 5, 4, 5,\n",
       "       4, 5, 9, 9, 4, 9, 5, 4, 9, 5, 9, 9, 9, 4, 4, 9, 9, 9, 9, 4, 9, 4,\n",
       "       9, 4, 4, 9, 9, 5, 9, 5, 4, 9, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(y_test, batch_size=30)\n",
    "model.predict_classes(y_test, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.09867053 0.09811851 0.10020258 0.10107579 0.09616145 0.09697772\n",
      "  0.09885885 0.1104214  0.103627   0.09588612]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.10206428 0.10330041 0.09870792 0.09740433 0.10705594 0.10431463\n",
      "  0.10164049 0.08623282 0.09240185 0.10687736]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10206428 0.10330041 0.09870792 0.09740433 0.10705594 0.10431463\n",
      "  0.10164049 0.08623282 0.09240185 0.10687736]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253247\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.09960989 0.09949061 0.09992879 0.10024744 0.09887736 0.09886298\n",
      "  0.09964871 0.10390862 0.10080178 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.10206428 0.10330041 0.09870792 0.09740433 0.10705594 0.10431463\n",
      "  0.10164049 0.08623282 0.09240185 0.10687736]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.10206428 0.10330041 0.09870792 0.09740433 0.10705594 0.10431463\n",
      "  0.10164049 0.08623282 0.09240185 0.10687736]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.10206428 0.10330041 0.09870792 0.09740433 0.10705594 0.10431463\n",
      "  0.10164049 0.08623282 0.09240185 0.10687736]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.10048839 0.10081189 0.09958656 0.09935688 0.10159941 0.10071494\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.10206428 0.10330041 0.09870792 0.09740433 0.10705594 0.10431463\n",
      "  0.10164049 0.08623281 0.09240184 0.10687736]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.09867053 0.09811851 0.10020258 0.10107579 0.09616145 0.09697772\n",
      "  0.09885885 0.1104214  0.103627   0.09588612]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.10206428 0.10330041 0.09870792 0.09740433 0.10705594 0.10431463\n",
      "  0.10164049 0.08623282 0.09240185 0.10687736]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.09867053 0.09811851 0.10020258 0.10107579 0.09616145 0.09697772\n",
      "  0.09885885 0.1104214  0.103627   0.09588612]\n",
      " [0.10048839 0.10081188 0.09958655 0.09935687 0.10159941 0.10071493\n",
      "  0.10037519 0.09771206 0.09798554 0.10136919]\n",
      " [0.10206428 0.10330041 0.09870792 0.09740433 0.10705594 0.10431463\n",
      "  0.10164049 0.08623282 0.09240185 0.10687736]\n",
      " [0.10130636 0.10208189 0.09917866 0.0984079  0.10432606 0.10253246\n",
      "  0.10103885 0.09182316 0.09518383 0.10412084]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]\n",
      " [0.10206428 0.10330041 0.09870792 0.09740433 0.10705594 0.10431463\n",
      "  0.10164049 0.08623282 0.09240185 0.10687736]\n",
      " [0.09960989 0.09949062 0.09992879 0.10024745 0.09887737 0.09886299\n",
      "  0.09964871 0.10390861 0.10080179 0.09862378]]\n"
     ]
    }
   ],
   "source": [
    "#Predict values test objects\n",
    "X_new = X_test\n",
    "y_proba = model.predict(X_new) # do I need the .round(2)?\n",
    "print(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "print(len(y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  [[4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [6.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]]\n",
      "y_train:  [[ 0.05      ]\n",
      " [ 0.15918367]\n",
      " [ 0.        ]\n",
      " [ 0.16241497]\n",
      " [ 0.        ]\n",
      " [ 0.15642857]\n",
      " [-0.09270833]\n",
      " [-0.06452991]\n",
      " [ 0.01012397]\n",
      " [ 0.43333333]\n",
      " [-0.02916667]\n",
      " [ 0.109375  ]\n",
      " [ 0.05333333]\n",
      " [ 0.1       ]\n",
      " [-0.01458333]\n",
      " [ 0.0795068 ]\n",
      " [ 0.09      ]\n",
      " [ 0.17857143]\n",
      " [ 0.275     ]\n",
      " [ 0.06009091]\n",
      " [ 0.2625    ]\n",
      " [-0.01916667]\n",
      " [ 0.09166667]\n",
      " [ 0.27559524]\n",
      " [ 0.13318182]\n",
      " [ 0.5       ]\n",
      " [ 0.19622024]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.21785714]\n",
      " [ 0.25      ]\n",
      " [-0.10833333]\n",
      " [ 0.15625   ]\n",
      " [ 0.07666667]\n",
      " [ 0.2625    ]\n",
      " [ 0.22222222]\n",
      " [ 0.0625    ]\n",
      " [-0.08636364]\n",
      " [ 0.18083333]\n",
      " [ 0.        ]\n",
      " [ 0.09970238]\n",
      " [ 0.22166667]\n",
      " [ 0.095     ]\n",
      " [ 0.08848485]\n",
      " [ 0.09333333]\n",
      " [ 0.07238095]\n",
      " [-0.175     ]\n",
      " [ 0.03541667]\n",
      " [ 0.17424242]\n",
      " [ 0.11      ]\n",
      " [ 0.09583333]\n",
      " [ 0.16805556]\n",
      " [ 0.06041667]\n",
      " [ 0.19410173]\n",
      " [ 0.        ]\n",
      " [ 0.28541667]\n",
      " [ 0.25      ]\n",
      " [ 0.16369048]\n",
      " [ 0.14166667]\n",
      " [ 0.21875   ]\n",
      " [ 0.2       ]\n",
      " [ 0.18050964]\n",
      " [ 0.29166667]\n",
      " [ 0.        ]\n",
      " [-0.02588745]\n",
      " [ 0.23224638]\n",
      " [ 0.        ]\n",
      " [ 0.125     ]\n",
      " [ 0.07935606]\n",
      " [ 0.15      ]\n",
      " [ 0.25      ]\n",
      " [ 0.025     ]\n",
      " [ 0.12      ]\n",
      " [-0.078125  ]\n",
      " [-0.071875  ]\n",
      " [-0.08518518]\n",
      " [-1.        ]\n",
      " [ 0.14895833]\n",
      " [ 0.01333333]\n",
      " [ 0.2125    ]\n",
      " [ 0.44      ]\n",
      " [ 0.0125    ]\n",
      " [ 0.06944444]\n",
      " [ 0.12857143]\n",
      " [ 0.04375   ]\n",
      " [ 0.23459208]\n",
      " [ 0.13928571]\n",
      " [ 0.6       ]\n",
      " [ 0.515     ]\n",
      " [-0.13645833]\n",
      " [ 0.25208333]\n",
      " [ 0.28928571]\n",
      " [ 0.13125   ]\n",
      " [ 0.10476191]\n",
      " [ 0.2       ]\n",
      " [ 0.22857143]\n",
      " [ 0.21808036]\n",
      " [ 0.5       ]\n",
      " [ 0.01882716]\n",
      " [ 0.40333333]\n",
      " [ 0.1       ]\n",
      " [ 0.45833333]\n",
      " [ 0.18095238]\n",
      " [ 0.04791667]\n",
      " [ 0.07058823]\n",
      " [ 0.06924242]\n",
      " [ 0.02222222]\n",
      " [ 0.03629704]\n",
      " [-0.15555556]\n",
      " [-0.05      ]\n",
      " [ 0.23579545]\n",
      " [-0.5       ]\n",
      " [ 0.00449495]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.13863636]\n",
      " [-0.034375  ]\n",
      " [ 0.        ]\n",
      " [-0.66666667]\n",
      " [ 0.14583333]\n",
      " [ 0.10871212]\n",
      " [ 0.15659341]\n",
      " [-0.02630386]\n",
      " [-0.00779221]\n",
      " [ 0.6       ]\n",
      " [-0.125     ]\n",
      " [ 0.13333333]\n",
      " [ 0.25      ]\n",
      " [ 0.06966667]\n",
      " [ 0.1       ]\n",
      " [ 0.0375    ]\n",
      " [ 0.00595238]\n",
      " [ 0.01871693]\n",
      " [ 0.10548611]\n",
      " [ 0.259375  ]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.11470058]\n",
      " [ 0.06471861]\n",
      " [ 0.1694697 ]\n",
      " [ 0.20558036]\n",
      " [ 0.52      ]\n",
      " [ 0.06595238]\n",
      " [ 0.075     ]\n",
      " [ 0.22857143]\n",
      " [-0.03333333]\n",
      " [ 0.06666667]\n",
      " [-0.01458333]\n",
      " [ 0.1527972 ]\n",
      " [ 0.11071429]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.05277778]\n",
      " [-0.046875  ]\n",
      " [-0.00333333]\n",
      " [ 0.18229167]\n",
      " [ 0.0476087 ]\n",
      " [ 0.28214286]\n",
      " [ 0.0875    ]\n",
      " [-0.07777778]\n",
      " [ 0.26944444]\n",
      " [ 0.01      ]\n",
      " [-0.04466667]\n",
      " [ 0.14404762]\n",
      " [ 0.45      ]\n",
      " [ 0.09761905]\n",
      " [ 0.175     ]\n",
      " [-0.03333333]\n",
      " [ 0.45      ]\n",
      " [ 0.06428571]\n",
      " [ 0.1       ]\n",
      " [ 0.20767196]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.26346154]\n",
      " [ 0.14      ]\n",
      " [ 0.45833333]\n",
      " [-0.125     ]\n",
      " [ 0.11583333]\n",
      " [ 0.00476431]\n",
      " [ 0.17777778]\n",
      " [ 0.15833333]\n",
      " [-0.14861111]\n",
      " [-0.16666667]\n",
      " [ 0.06944444]\n",
      " [ 0.20857143]\n",
      " [ 0.05404762]\n",
      " [-0.04378157]\n",
      " [ 0.15170068]\n",
      " [ 0.125     ]\n",
      " [ 0.08374242]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.25909091]\n",
      " [ 0.185     ]\n",
      " [ 0.03592558]\n",
      " [ 0.22777778]\n",
      " [ 0.11308081]\n",
      " [ 0.17291667]\n",
      " [-0.25      ]\n",
      " [-0.65      ]\n",
      " [-0.11777778]\n",
      " [ 0.02380952]\n",
      " [ 0.        ]\n",
      " [ 0.30833333]\n",
      " [-0.015     ]\n",
      " [ 0.09393939]\n",
      " [ 0.0625    ]\n",
      " [ 0.13809524]\n",
      " [ 0.15333333]\n",
      " [-0.04356061]\n",
      " [ 0.09402597]\n",
      " [ 0.2147549 ]\n",
      " [ 0.3       ]\n",
      " [ 0.20189349]\n",
      " [ 0.28333333]\n",
      " [-0.35      ]\n",
      " [-0.1075    ]\n",
      " [ 0.45      ]\n",
      " [ 0.10555556]\n",
      " [ 0.3       ]\n",
      " [ 0.07052083]\n",
      " [ 0.1225    ]\n",
      " [ 0.17083333]\n",
      " [ 0.2       ]\n",
      " [ 0.06428571]\n",
      " [ 0.02579365]\n",
      " [-1.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.28583333]\n",
      " [ 0.16176471]\n",
      " [ 0.23333333]\n",
      " [ 0.01369048]\n",
      " [-0.45      ]\n",
      " [ 0.27575758]\n",
      " [ 0.21666667]\n",
      " [ 0.01111111]\n",
      " [ 0.08416667]\n",
      " [-0.01333333]\n",
      " [ 0.19284512]\n",
      " [ 0.16190476]\n",
      " [ 0.01715007]\n",
      " [ 0.01875   ]\n",
      " [ 0.15      ]\n",
      " [ 0.        ]\n",
      " [ 0.04785902]\n",
      " [ 0.20166667]\n",
      " [ 0.17142857]\n",
      " [ 0.28958333]\n",
      " [-0.04523809]\n",
      " [-0.01464646]\n",
      " [ 0.0625    ]\n",
      " [ 0.02166667]\n",
      " [ 0.4       ]\n",
      " [-0.03916667]\n",
      " [ 0.5       ]\n",
      " [-0.55      ]\n",
      " [ 0.02060606]\n",
      " [ 0.10833333]\n",
      " [ 0.14526515]\n",
      " [ 0.275     ]\n",
      " [ 0.16666667]\n",
      " [ 0.55      ]\n",
      " [ 0.05552083]\n",
      " [-0.06372549]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.01277778]\n",
      " [ 0.28888889]\n",
      " [ 0.375     ]\n",
      " [-0.31833333]\n",
      " [ 0.46666667]]\n",
      "X_test:  [[3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]]\n",
      "y_test:  [[ 0.275     ]\n",
      " [ 0.00555556]\n",
      " [ 0.13076923]\n",
      " [ 0.10504329]\n",
      " [ 0.25694444]\n",
      " [ 0.09285714]\n",
      " [-0.04166667]\n",
      " [ 0.125     ]\n",
      " [ 0.09675698]\n",
      " [ 0.5       ]\n",
      " [-0.01186526]\n",
      " [ 0.26666667]\n",
      " [ 0.06435185]\n",
      " [ 0.19027778]\n",
      " [-0.4       ]\n",
      " [ 0.18450635]\n",
      " [ 0.15      ]\n",
      " [ 0.24404762]\n",
      " [ 0.34545455]\n",
      " [-0.025     ]\n",
      " [ 0.2125    ]\n",
      " [ 0.1030303 ]\n",
      " [ 0.44444444]\n",
      " [-0.55555556]\n",
      " [ 0.4       ]\n",
      " [ 0.2765873 ]\n",
      " [ 0.13026245]\n",
      " [ 0.02453704]\n",
      " [ 0.15948162]\n",
      " [ 0.33888889]\n",
      " [ 0.08458333]\n",
      " [ 0.20160256]\n",
      " [ 0.121875  ]\n",
      " [ 0.19820513]\n",
      " [ 0.06428571]\n",
      " [ 0.09419643]\n",
      " [ 0.19888889]\n",
      " [ 0.1537037 ]\n",
      " [ 0.15208333]\n",
      " [ 0.        ]\n",
      " [ 0.38333333]\n",
      " [-0.2       ]\n",
      " [ 0.28571429]\n",
      " [-0.225     ]\n",
      " [ 0.33333333]\n",
      " [-0.29166667]\n",
      " [ 0.05769231]\n",
      " [ 0.01944444]\n",
      " [ 0.675     ]\n",
      " [ 0.09545455]\n",
      " [-0.17638889]\n",
      " [ 0.46666667]\n",
      " [ 0.07967607]\n",
      " [-0.2       ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.06527778]\n",
      " [ 0.21645833]\n",
      " [ 0.2575    ]\n",
      " [ 0.12380952]\n",
      " [ 0.01636905]\n",
      " [ 0.        ]\n",
      " [ 0.18333333]\n",
      " [ 0.25      ]\n",
      " [ 0.1625    ]\n",
      " [ 0.56666667]\n",
      " [ 0.155     ]\n",
      " [ 0.44      ]\n",
      " [ 0.3       ]\n",
      " [-0.01214286]\n",
      " [ 0.07142857]\n",
      " [-0.3625    ]\n",
      " [ 0.0625    ]\n",
      " [-0.31666667]\n",
      " [ 0.47916667]\n",
      " [ 0.06666667]\n",
      " [-0.20833333]\n",
      " [ 0.        ]\n",
      " [ 0.17381245]\n",
      " [ 0.07795056]\n",
      " [-0.14583333]\n",
      " [ 0.03125   ]\n",
      " [ 0.04494949]\n",
      " [ 0.11909091]\n",
      " [ 0.07944444]\n",
      " [-0.0625    ]\n",
      " [-0.1       ]\n",
      " [ 0.01136364]\n",
      " [ 0.10277778]\n",
      " [ 0.02777778]\n",
      " [ 0.        ]\n",
      " [ 0.08863636]]\n"
     ]
    }
   ],
   "source": [
    "# now let's try a prediction based on Q40\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = model_selection.train_test_split(NLPdata[:,1], NLPdata[:,2], train_size=0.75,test_size=0.25, random_state=101)\n",
    "print (\"X_train: \", X_train)\n",
    "print (\"y_train: \", y_train)\n",
    "print (\"X_test: \", X_test)\n",
    "print (\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0398 - val_loss: 0.0428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14092f280>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train2, y_train2, batch_size=10, epochs=10, verbose=1, validation_data=(X_test2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04280411824584007"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test2, y_test2, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 9, 4, 7, 4, 9, 4, 9, 7, 4, 4, 9, 9, 4, 4, 9, 9, 4, 4, 9, 7, 7,\n",
       "       4, 4, 7, 7, 4, 7, 9, 4, 9, 1, 4, 4, 4, 4, 4, 4, 9, 7, 9, 4, 9, 9,\n",
       "       4, 7, 9, 4, 1, 4, 4, 4, 4, 9, 9, 4, 7, 7, 7, 9, 4, 9, 7, 7, 4, 9,\n",
       "       9, 4, 4, 9, 7, 9, 9, 7, 9, 4, 4, 9, 9, 7, 4, 7, 9, 9, 9, 4, 4, 4,\n",
       "       4, 9, 7, 7])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test2, batch_size=10)\n",
    "model.predict_classes(X_test2, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 9, 9, 9, 4, 9, 5, 9, 9, 4, 9, 4, 9, 4, 2, 4, 9, 4, 4, 9, 4, 9,\n",
       "       4, 2, 4, 4, 9, 9, 4, 4, 9, 4, 9, 4, 9, 9, 4, 4, 9, 9, 4, 5, 4, 5,\n",
       "       4, 2, 9, 9, 4, 9, 5, 4, 9, 5, 9, 9, 5, 4, 4, 9, 9, 9, 4, 4, 4, 4,\n",
       "       4, 4, 4, 9, 9, 2, 9, 2, 4, 9, 5, 9, 4, 9, 5, 9, 9, 9, 9, 5, 5, 9,\n",
       "       9, 9, 9, 9])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(y_test2, batch_size=10)\n",
    "model.predict_classes(y_test2, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.0989629  0.0985764  0.10003351 0.10033737 0.09750503 0.09817898\n",
      "  0.09923381 0.10880693 0.10137435 0.09699067]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.0989629  0.0985764  0.10003351 0.10033737 0.09750503 0.09817898\n",
      "  0.09923381 0.10880693 0.10137435 0.09699067]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.10106185 0.10147966 0.10001903 0.10022839 0.1015517  0.10078646\n",
      "  0.10036168 0.09412455 0.09928028 0.10110639]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10106185 0.10147966 0.10001903 0.10022839 0.1015517  0.10078646\n",
      "  0.10036168 0.09412455 0.09928028 0.10110639]\n",
      " [0.10106185 0.10147966 0.10001903 0.10022839 0.1015517  0.10078646\n",
      "  0.10036168 0.09412455 0.09928028 0.10110639]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.0989629  0.0985764  0.10003351 0.10033737 0.09750503 0.09817898\n",
      "  0.09923381 0.10880693 0.10137435 0.09699067]\n",
      " [0.09821189 0.09756654 0.09997615 0.10031132 0.09613249 0.09726439\n",
      "  0.09879922 0.11412244 0.10201869 0.09559687]\n",
      " [0.10106185 0.10147966 0.10001903 0.10022839 0.1015517  0.10078646\n",
      "  0.10036168 0.09412455 0.09928028 0.10110639]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.09821189 0.09756654 0.09997615 0.10031132 0.09613249 0.09726439\n",
      "  0.09879922 0.11412244 0.10201869 0.09559687]\n",
      " [0.09821189 0.09756654 0.09997615 0.10031132 0.09613249 0.09726439\n",
      "  0.09879922 0.11412244 0.10201869 0.09559687]\n",
      " [0.10106185 0.10147966 0.10001903 0.10022839 0.1015517  0.10078646\n",
      "  0.10036168 0.09412455 0.09928028 0.10110639]\n",
      " [0.0989629  0.0985764  0.10003351 0.10033737 0.09750503 0.09817898\n",
      "  0.09923381 0.10880693 0.10137435 0.09699067]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10038743 0.1005327  0.10005388 0.10029481 0.10021456 0.09993967\n",
      "  0.10001431 0.09881385 0.10000346 0.09974536]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.10106185 0.10147966 0.10001903 0.10022839 0.1015517  0.10078646\n",
      "  0.10036168 0.09412455 0.09928028 0.10110639]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.0989629  0.0985764  0.10003351 0.10033737 0.09750503 0.09817898\n",
      "  0.09923381 0.10880693 0.10137435 0.09699067]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.0989629  0.0985764  0.10003351 0.10033737 0.09750503 0.09817898\n",
      "  0.09923381 0.10880693 0.10137435 0.09699067]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.10038742 0.1005327  0.10005388 0.1002948  0.10021456 0.09993968\n",
      "  0.10001431 0.09881384 0.10000346 0.09974536]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.10106185 0.10147966 0.10001903 0.10022839 0.1015517  0.10078646\n",
      "  0.10036168 0.09412455 0.09928028 0.10110639]\n",
      " [0.10106185 0.10147966 0.10001903 0.10022839 0.1015517  0.10078646\n",
      "  0.10036168 0.09412455 0.09928028 0.10110639]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10106185 0.10147966 0.10001903 0.10022839 0.1015517  0.10078646\n",
      "  0.10036168 0.09412455 0.09928028 0.10110639]\n",
      " [0.0989629  0.0985764  0.10003351 0.10033737 0.09750503 0.09817898\n",
      "  0.09923381 0.10880693 0.10137435 0.09699067]\n",
      " [0.09821189 0.09756654 0.09997615 0.10031132 0.09613249 0.09726439\n",
      "  0.09879922 0.11412244 0.10201869 0.09559687]\n",
      " [0.09821189 0.09756654 0.09997615 0.10031132 0.09613249 0.09726439\n",
      "  0.09879922 0.11412244 0.10201869 0.09559687]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.0989629  0.0985764  0.10003351 0.10033736 0.09750504 0.09817898\n",
      "  0.09923382 0.10880695 0.10137435 0.09699066]\n",
      " [0.0989629  0.0985764  0.10003351 0.10033736 0.09750504 0.09817898\n",
      "  0.09923382 0.10880695 0.10137435 0.09699066]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.10106185 0.10147966 0.10001903 0.10022839 0.1015517  0.10078646\n",
      "  0.10036168 0.09412455 0.09928028 0.10110639]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.0989629  0.0985764  0.10003351 0.10033737 0.09750503 0.09817898\n",
      "  0.09923381 0.10880693 0.10137435 0.09699067]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.09821189 0.09756654 0.09997615 0.10031132 0.09613249 0.09726439\n",
      "  0.09879922 0.11412244 0.10201869 0.09559687]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.10106185 0.10147966 0.10001903 0.10022839 0.1015517  0.10078646\n",
      "  0.10036168 0.09412455 0.09928028 0.10110639]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.0989629  0.0985764  0.10003351 0.10033737 0.09750503 0.09817898\n",
      "  0.09923381 0.10880693 0.10137435 0.09699067]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.09968793 0.09956505 0.10005908 0.10033149 0.0988657  0.09907065\n",
      "  0.0996386  0.10370601 0.10070203 0.0983735 ]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.10106185 0.10147966 0.10001903 0.10022839 0.1015517  0.10078646\n",
      "  0.10036168 0.09412455 0.09928028 0.10110639]\n",
      " [0.10171172 0.10240626 0.0999556  0.1001334  0.10287727 0.10161135\n",
      "  0.10068144 0.08963215 0.09853414 0.10245668]\n",
      " [0.09985018 0.0992253  0.10083149 0.09952573 0.10152185 0.10302215\n",
      "  0.10120422 0.091053   0.09977868 0.10398736]\n",
      " [0.09821189 0.09756654 0.09997615 0.10031132 0.09613249 0.09726439\n",
      "  0.09879922 0.11412244 0.10201869 0.09559687]\n",
      " [0.0989629  0.0985764  0.10003351 0.10033737 0.09750503 0.09817898\n",
      "  0.09923381 0.10880693 0.10137435 0.09699067]]\n"
     ]
    }
   ],
   "source": [
    "#Predict values test objects\n",
    "X_new = X_test2\n",
    "y_proba = model.predict(X_new) # do I need the .round(2)?\n",
    "print(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict values test objects\n",
    "X_new = X_test2\n",
    "y_proba = model.predict(X_new)\n",
    "print(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1406b9760>,\n",
       " <matplotlib.lines.Line2D at 0x1406b9b80>,\n",
       " <matplotlib.lines.Line2D at 0x1406b98b0>,\n",
       " <matplotlib.lines.Line2D at 0x1406b9c10>,\n",
       " <matplotlib.lines.Line2D at 0x1406b97c0>,\n",
       " <matplotlib.lines.Line2D at 0x14067c9a0>,\n",
       " <matplotlib.lines.Line2D at 0x14067c520>,\n",
       " <matplotlib.lines.Line2D at 0x140650eb0>,\n",
       " <matplotlib.lines.Line2D at 0x1406b9220>,\n",
       " <matplotlib.lines.Line2D at 0x1406b9490>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0,92,92)\n",
    "y1 = y_test\n",
    "y2 = y_proba\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(x,y1, 'x')\n",
    "plt.plot(x,y2, '*', c='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLPdata = pd.read_csv('nlpdata.csv')\n",
    "NLPdata = NLPdata.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, activation='selu', input_dim=1)) \n",
    "#model.add(Dense(28, activation='relu')) \n",
    "#model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  [4. 4. 5. 4. 4. 4. 4. 4. 3. 1. 4. 4. 3. 4. 4. 4. 4. 4. 4. 2. 4. 4. 4. 3.\n",
      " 1. 2. 3. 1. 4. 4. 6. 4. 4. 1. 4. 4. 1. 4. 4. 2. 3. 4. 4. 2. 1. 5. 3. 2.\n",
      " 5. 4. 4. 2. 4. 4. 1. 1. 4. 4. 2. 4. 2. 3. 2. 4. 4. 4. 4. 4. 4. 3. 6. 3.\n",
      " 3. 4. 3. 4. 2. 5. 2. 1. 4. 1. 4. 3. 4. 1. 4. 1. 3. 1. 3. 3. 3. 4. 4. 5.\n",
      " 4. 4. 3. 4. 4. 4. 4. 4. 2. 2. 4. 4. 4. 4. 4. 1. 2. 3. 2. 4. 4. 4. 1. 4.\n",
      " 5. 1. 5. 4. 4. 2. 4. 3. 2. 4. 3. 3. 4. 1. 3. 5. 3. 2. 3. 5. 4. 4. 4. 4.\n",
      " 3. 1. 4. 3. 2. 2. 4. 2. 4. 4. 4. 4. 2. 4. 4. 3. 4. 1. 4. 4. 4. 3. 4. 4.\n",
      " 2. 2. 2. 4. 4. 3. 4. 3. 4. 4. 2. 2. 4. 4. 5. 1. 2. 4. 4. 2. 2. 4. 3. 4.\n",
      " 4. 4. 5. 4. 2. 3. 1. 4. 3. 4. 4. 3. 2. 4. 1. 4. 4. 2. 4. 4. 4. 4. 4. 4.\n",
      " 2. 2. 4. 2. 4. 4. 4. 5. 4. 3. 4. 4. 3. 2. 1. 3. 4. 4. 3. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 1. 4. 4. 3. 1. 3. 4. 4. 4. 4. 4. 4. 2. 4. 2. 3.\n",
      " 3. 4. 2. 4. 4. 4. 4. 4. 1.]\n",
      "y_train:  [ 0.05        0.15918367  0.          0.16241497  0.          0.15642857\n",
      " -0.09270833 -0.06452991  0.01012397  0.43333333 -0.02916667  0.109375\n",
      "  0.05333333  0.1        -0.01458333  0.0795068   0.09        0.17857143\n",
      "  0.275       0.06009091  0.2625     -0.01916667  0.09166667  0.27559524\n",
      "  0.13318182  0.5         0.19622024  0.          0.2         0.21785714\n",
      "  0.25       -0.10833333  0.15625     0.07666667  0.2625      0.22222222\n",
      "  0.0625     -0.08636364  0.18083333  0.          0.09970238  0.22166667\n",
      "  0.095       0.08848485  0.09333333  0.07238095 -0.175       0.03541667\n",
      "  0.17424242  0.11        0.09583333  0.16805556  0.06041667  0.19410173\n",
      "  0.          0.28541667  0.25        0.16369048  0.14166667  0.21875\n",
      "  0.2         0.18050964  0.29166667  0.         -0.02588745  0.23224638\n",
      "  0.          0.125       0.07935606  0.15        0.25        0.025\n",
      "  0.12       -0.078125   -0.071875   -0.08518518 -1.          0.14895833\n",
      "  0.01333333  0.2125      0.44        0.0125      0.06944444  0.12857143\n",
      "  0.04375     0.23459208  0.13928571  0.6         0.515      -0.13645833\n",
      "  0.25208333  0.28928571  0.13125     0.10476191  0.2         0.22857143\n",
      "  0.21808036  0.5         0.01882716  0.40333333  0.1         0.45833333\n",
      "  0.18095238  0.04791667  0.07058823  0.06924242  0.02222222  0.03629704\n",
      " -0.15555556 -0.05        0.23579545 -0.5         0.00449495  0.16666667\n",
      "  0.          0.13863636 -0.034375    0.         -0.66666667  0.14583333\n",
      "  0.10871212  0.15659341 -0.02630386 -0.00779221  0.6        -0.125\n",
      "  0.13333333  0.25        0.06966667  0.1         0.0375      0.00595238\n",
      "  0.01871693  0.10548611  0.259375    0.          0.2         0.11470058\n",
      "  0.06471861  0.1694697   0.20558036  0.52        0.06595238  0.075\n",
      "  0.22857143 -0.03333333  0.06666667 -0.01458333  0.1527972   0.11071429\n",
      "  0.          0.          0.05277778 -0.046875   -0.00333333  0.18229167\n",
      "  0.0476087   0.28214286  0.0875     -0.07777778  0.26944444  0.01\n",
      " -0.04466667  0.14404762  0.45        0.09761905  0.175      -0.03333333\n",
      "  0.45        0.06428571  0.1         0.20767196  0.          0.\n",
      "  0.26346154  0.14        0.45833333 -0.125       0.11583333  0.00476431\n",
      "  0.17777778  0.15833333 -0.14861111 -0.16666667  0.06944444  0.20857143\n",
      "  0.05404762 -0.04378157  0.15170068  0.125       0.08374242  0.16666667\n",
      "  0.          0.25        0.25909091  0.185       0.03592558  0.22777778\n",
      "  0.11308081  0.17291667 -0.25       -0.65       -0.11777778  0.02380952\n",
      "  0.          0.30833333 -0.015       0.09393939  0.0625      0.13809524\n",
      "  0.15333333 -0.04356061  0.09402597  0.2147549   0.3         0.20189349\n",
      "  0.28333333 -0.35       -0.1075      0.45        0.10555556  0.3\n",
      "  0.07052083  0.1225      0.17083333  0.2         0.06428571  0.02579365\n",
      " -1.          0.25        0.28583333  0.16176471  0.23333333  0.01369048\n",
      " -0.45        0.27575758  0.21666667  0.01111111  0.08416667 -0.01333333\n",
      "  0.19284512  0.16190476  0.01715007  0.01875     0.15        0.\n",
      "  0.04785902  0.20166667  0.17142857  0.28958333 -0.04523809 -0.01464646\n",
      "  0.0625      0.02166667  0.4        -0.03916667  0.5        -0.55\n",
      "  0.02060606  0.10833333  0.14526515  0.275       0.16666667  0.55\n",
      "  0.05552083 -0.06372549  0.          0.         -0.01277778  0.28888889\n",
      "  0.375      -0.31833333  0.46666667]\n",
      "X_test:  [3. 3. 4. 2. 4. 3. 4. 2. 3. 4. 3. 3. 5. 4. 4. 4. 4. 4. 4. 2. 1. 2. 4. 4.\n",
      " 4. 4. 4. 1. 4. 2. 2. 4. 4. 3. 1. 2. 2. 1. 2. 2. 1. 2. 3. 4. 4. 4. 4. 4.\n",
      " 4. 2. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 3. 3. 1. 3. 4. 4. 2. 4. 3. 2. 4.\n",
      " 3. 5. 4. 2. 1. 3. 4. 4. 4. 4. 3. 4. 4. 5. 3. 1. 2. 4. 1. 4.]\n",
      "y_test:  [ 0.275       0.00555556  0.13076923  0.10504329  0.25694444  0.09285714\n",
      " -0.04166667  0.125       0.09675698  0.5        -0.01186526  0.26666667\n",
      "  0.06435185  0.19027778 -0.4         0.18450635  0.15        0.24404762\n",
      "  0.34545455 -0.025       0.2125      0.1030303   0.44444444 -0.55555556\n",
      "  0.4         0.2765873   0.13026245  0.02453704  0.15948162  0.33888889\n",
      "  0.08458333  0.20160256  0.121875    0.19820513  0.06428571  0.09419643\n",
      "  0.19888889  0.1537037   0.15208333  0.          0.38333333 -0.2\n",
      "  0.28571429 -0.225       0.33333333 -0.29166667  0.05769231  0.01944444\n",
      "  0.675       0.09545455 -0.17638889  0.46666667  0.07967607 -0.2\n",
      "  0.          0.         -0.06527778  0.21645833  0.2575      0.12380952\n",
      "  0.01636905  0.          0.18333333  0.25        0.1625      0.56666667\n",
      "  0.155       0.44        0.3        -0.01214286  0.07142857 -0.3625\n",
      "  0.0625     -0.31666667  0.47916667  0.06666667 -0.20833333  0.\n",
      "  0.17381245  0.07795056 -0.14583333  0.03125     0.04494949  0.11909091\n",
      "  0.07944444 -0.0625     -0.1         0.01136364  0.10277778  0.02777778\n",
      "  0.          0.08863636]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection as model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(NLPdata[:,0], NLPdata[:,2], train_size=0.75,test_size=0.25, random_state=101)\n",
    "print (\"X_train: \", X_train)\n",
    "print (\"y_train: \", y_train)\n",
    "print (\"X_test: \", X_test)\n",
    "print (\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  [[4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [6.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]]\n",
      "y_train:  [[ 0.05      ]\n",
      " [ 0.15918367]\n",
      " [ 0.        ]\n",
      " [ 0.16241497]\n",
      " [ 0.        ]\n",
      " [ 0.15642857]\n",
      " [-0.09270833]\n",
      " [-0.06452991]\n",
      " [ 0.01012397]\n",
      " [ 0.43333333]\n",
      " [-0.02916667]\n",
      " [ 0.109375  ]\n",
      " [ 0.05333333]\n",
      " [ 0.1       ]\n",
      " [-0.01458333]\n",
      " [ 0.0795068 ]\n",
      " [ 0.09      ]\n",
      " [ 0.17857143]\n",
      " [ 0.275     ]\n",
      " [ 0.06009091]\n",
      " [ 0.2625    ]\n",
      " [-0.01916667]\n",
      " [ 0.09166667]\n",
      " [ 0.27559524]\n",
      " [ 0.13318182]\n",
      " [ 0.5       ]\n",
      " [ 0.19622024]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.21785714]\n",
      " [ 0.25      ]\n",
      " [-0.10833333]\n",
      " [ 0.15625   ]\n",
      " [ 0.07666667]\n",
      " [ 0.2625    ]\n",
      " [ 0.22222222]\n",
      " [ 0.0625    ]\n",
      " [-0.08636364]\n",
      " [ 0.18083333]\n",
      " [ 0.        ]\n",
      " [ 0.09970238]\n",
      " [ 0.22166667]\n",
      " [ 0.095     ]\n",
      " [ 0.08848485]\n",
      " [ 0.09333333]\n",
      " [ 0.07238095]\n",
      " [-0.175     ]\n",
      " [ 0.03541667]\n",
      " [ 0.17424242]\n",
      " [ 0.11      ]\n",
      " [ 0.09583333]\n",
      " [ 0.16805556]\n",
      " [ 0.06041667]\n",
      " [ 0.19410173]\n",
      " [ 0.        ]\n",
      " [ 0.28541667]\n",
      " [ 0.25      ]\n",
      " [ 0.16369048]\n",
      " [ 0.14166667]\n",
      " [ 0.21875   ]\n",
      " [ 0.2       ]\n",
      " [ 0.18050964]\n",
      " [ 0.29166667]\n",
      " [ 0.        ]\n",
      " [-0.02588745]\n",
      " [ 0.23224638]\n",
      " [ 0.        ]\n",
      " [ 0.125     ]\n",
      " [ 0.07935606]\n",
      " [ 0.15      ]\n",
      " [ 0.25      ]\n",
      " [ 0.025     ]\n",
      " [ 0.12      ]\n",
      " [-0.078125  ]\n",
      " [-0.071875  ]\n",
      " [-0.08518518]\n",
      " [-1.        ]\n",
      " [ 0.14895833]\n",
      " [ 0.01333333]\n",
      " [ 0.2125    ]\n",
      " [ 0.44      ]\n",
      " [ 0.0125    ]\n",
      " [ 0.06944444]\n",
      " [ 0.12857143]\n",
      " [ 0.04375   ]\n",
      " [ 0.23459208]\n",
      " [ 0.13928571]\n",
      " [ 0.6       ]\n",
      " [ 0.515     ]\n",
      " [-0.13645833]\n",
      " [ 0.25208333]\n",
      " [ 0.28928571]\n",
      " [ 0.13125   ]\n",
      " [ 0.10476191]\n",
      " [ 0.2       ]\n",
      " [ 0.22857143]\n",
      " [ 0.21808036]\n",
      " [ 0.5       ]\n",
      " [ 0.01882716]\n",
      " [ 0.40333333]\n",
      " [ 0.1       ]\n",
      " [ 0.45833333]\n",
      " [ 0.18095238]\n",
      " [ 0.04791667]\n",
      " [ 0.07058823]\n",
      " [ 0.06924242]\n",
      " [ 0.02222222]\n",
      " [ 0.03629704]\n",
      " [-0.15555556]\n",
      " [-0.05      ]\n",
      " [ 0.23579545]\n",
      " [-0.5       ]\n",
      " [ 0.00449495]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.13863636]\n",
      " [-0.034375  ]\n",
      " [ 0.        ]\n",
      " [-0.66666667]\n",
      " [ 0.14583333]\n",
      " [ 0.10871212]\n",
      " [ 0.15659341]\n",
      " [-0.02630386]\n",
      " [-0.00779221]\n",
      " [ 0.6       ]\n",
      " [-0.125     ]\n",
      " [ 0.13333333]\n",
      " [ 0.25      ]\n",
      " [ 0.06966667]\n",
      " [ 0.1       ]\n",
      " [ 0.0375    ]\n",
      " [ 0.00595238]\n",
      " [ 0.01871693]\n",
      " [ 0.10548611]\n",
      " [ 0.259375  ]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.11470058]\n",
      " [ 0.06471861]\n",
      " [ 0.1694697 ]\n",
      " [ 0.20558036]\n",
      " [ 0.52      ]\n",
      " [ 0.06595238]\n",
      " [ 0.075     ]\n",
      " [ 0.22857143]\n",
      " [-0.03333333]\n",
      " [ 0.06666667]\n",
      " [-0.01458333]\n",
      " [ 0.1527972 ]\n",
      " [ 0.11071429]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.05277778]\n",
      " [-0.046875  ]\n",
      " [-0.00333333]\n",
      " [ 0.18229167]\n",
      " [ 0.0476087 ]\n",
      " [ 0.28214286]\n",
      " [ 0.0875    ]\n",
      " [-0.07777778]\n",
      " [ 0.26944444]\n",
      " [ 0.01      ]\n",
      " [-0.04466667]\n",
      " [ 0.14404762]\n",
      " [ 0.45      ]\n",
      " [ 0.09761905]\n",
      " [ 0.175     ]\n",
      " [-0.03333333]\n",
      " [ 0.45      ]\n",
      " [ 0.06428571]\n",
      " [ 0.1       ]\n",
      " [ 0.20767196]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.26346154]\n",
      " [ 0.14      ]\n",
      " [ 0.45833333]\n",
      " [-0.125     ]\n",
      " [ 0.11583333]\n",
      " [ 0.00476431]\n",
      " [ 0.17777778]\n",
      " [ 0.15833333]\n",
      " [-0.14861111]\n",
      " [-0.16666667]\n",
      " [ 0.06944444]\n",
      " [ 0.20857143]\n",
      " [ 0.05404762]\n",
      " [-0.04378157]\n",
      " [ 0.15170068]\n",
      " [ 0.125     ]\n",
      " [ 0.08374242]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.25909091]\n",
      " [ 0.185     ]\n",
      " [ 0.03592558]\n",
      " [ 0.22777778]\n",
      " [ 0.11308081]\n",
      " [ 0.17291667]\n",
      " [-0.25      ]\n",
      " [-0.65      ]\n",
      " [-0.11777778]\n",
      " [ 0.02380952]\n",
      " [ 0.        ]\n",
      " [ 0.30833333]\n",
      " [-0.015     ]\n",
      " [ 0.09393939]\n",
      " [ 0.0625    ]\n",
      " [ 0.13809524]\n",
      " [ 0.15333333]\n",
      " [-0.04356061]\n",
      " [ 0.09402597]\n",
      " [ 0.2147549 ]\n",
      " [ 0.3       ]\n",
      " [ 0.20189349]\n",
      " [ 0.28333333]\n",
      " [-0.35      ]\n",
      " [-0.1075    ]\n",
      " [ 0.45      ]\n",
      " [ 0.10555556]\n",
      " [ 0.3       ]\n",
      " [ 0.07052083]\n",
      " [ 0.1225    ]\n",
      " [ 0.17083333]\n",
      " [ 0.2       ]\n",
      " [ 0.06428571]\n",
      " [ 0.02579365]\n",
      " [-1.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.28583333]\n",
      " [ 0.16176471]\n",
      " [ 0.23333333]\n",
      " [ 0.01369048]\n",
      " [-0.45      ]\n",
      " [ 0.27575758]\n",
      " [ 0.21666667]\n",
      " [ 0.01111111]\n",
      " [ 0.08416667]\n",
      " [-0.01333333]\n",
      " [ 0.19284512]\n",
      " [ 0.16190476]\n",
      " [ 0.01715007]\n",
      " [ 0.01875   ]\n",
      " [ 0.15      ]\n",
      " [ 0.        ]\n",
      " [ 0.04785902]\n",
      " [ 0.20166667]\n",
      " [ 0.17142857]\n",
      " [ 0.28958333]\n",
      " [-0.04523809]\n",
      " [-0.01464646]\n",
      " [ 0.0625    ]\n",
      " [ 0.02166667]\n",
      " [ 0.4       ]\n",
      " [-0.03916667]\n",
      " [ 0.5       ]\n",
      " [-0.55      ]\n",
      " [ 0.02060606]\n",
      " [ 0.10833333]\n",
      " [ 0.14526515]\n",
      " [ 0.275     ]\n",
      " [ 0.16666667]\n",
      " [ 0.55      ]\n",
      " [ 0.05552083]\n",
      " [-0.06372549]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.01277778]\n",
      " [ 0.28888889]\n",
      " [ 0.375     ]\n",
      " [-0.31833333]\n",
      " [ 0.46666667]]\n",
      "X_test:  [[3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]]\n",
      "y_test:  [[ 0.275     ]\n",
      " [ 0.00555556]\n",
      " [ 0.13076923]\n",
      " [ 0.10504329]\n",
      " [ 0.25694444]\n",
      " [ 0.09285714]\n",
      " [-0.04166667]\n",
      " [ 0.125     ]\n",
      " [ 0.09675698]\n",
      " [ 0.5       ]\n",
      " [-0.01186526]\n",
      " [ 0.26666667]\n",
      " [ 0.06435185]\n",
      " [ 0.19027778]\n",
      " [-0.4       ]\n",
      " [ 0.18450635]\n",
      " [ 0.15      ]\n",
      " [ 0.24404762]\n",
      " [ 0.34545455]\n",
      " [-0.025     ]\n",
      " [ 0.2125    ]\n",
      " [ 0.1030303 ]\n",
      " [ 0.44444444]\n",
      " [-0.55555556]\n",
      " [ 0.4       ]\n",
      " [ 0.2765873 ]\n",
      " [ 0.13026245]\n",
      " [ 0.02453704]\n",
      " [ 0.15948162]\n",
      " [ 0.33888889]\n",
      " [ 0.08458333]\n",
      " [ 0.20160256]\n",
      " [ 0.121875  ]\n",
      " [ 0.19820513]\n",
      " [ 0.06428571]\n",
      " [ 0.09419643]\n",
      " [ 0.19888889]\n",
      " [ 0.1537037 ]\n",
      " [ 0.15208333]\n",
      " [ 0.        ]\n",
      " [ 0.38333333]\n",
      " [-0.2       ]\n",
      " [ 0.28571429]\n",
      " [-0.225     ]\n",
      " [ 0.33333333]\n",
      " [-0.29166667]\n",
      " [ 0.05769231]\n",
      " [ 0.01944444]\n",
      " [ 0.675     ]\n",
      " [ 0.09545455]\n",
      " [-0.17638889]\n",
      " [ 0.46666667]\n",
      " [ 0.07967607]\n",
      " [-0.2       ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.06527778]\n",
      " [ 0.21645833]\n",
      " [ 0.2575    ]\n",
      " [ 0.12380952]\n",
      " [ 0.01636905]\n",
      " [ 0.        ]\n",
      " [ 0.18333333]\n",
      " [ 0.25      ]\n",
      " [ 0.1625    ]\n",
      " [ 0.56666667]\n",
      " [ 0.155     ]\n",
      " [ 0.44      ]\n",
      " [ 0.3       ]\n",
      " [-0.01214286]\n",
      " [ 0.07142857]\n",
      " [-0.3625    ]\n",
      " [ 0.0625    ]\n",
      " [-0.31666667]\n",
      " [ 0.47916667]\n",
      " [ 0.06666667]\n",
      " [-0.20833333]\n",
      " [ 0.        ]\n",
      " [ 0.17381245]\n",
      " [ 0.07795056]\n",
      " [-0.14583333]\n",
      " [ 0.03125   ]\n",
      " [ 0.04494949]\n",
      " [ 0.11909091]\n",
      " [ 0.07944444]\n",
      " [-0.0625    ]\n",
      " [-0.1       ]\n",
      " [ 0.01136364]\n",
      " [ 0.10277778]\n",
      " [ 0.02777778]\n",
      " [ 0.        ]\n",
      " [ 0.08863636]]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1,1)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "X_test = X_test.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print (\"X_train: \", X_train)\n",
    "print (\"y_train: \", y_train)\n",
    "print (\"X_test: \", X_test)\n",
    "print (\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0422 - val_loss: 0.0460\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0421 - val_loss: 0.0459\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.0459\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.0458\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.0457\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.0456\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0418 - val_loss: 0.0456\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0418 - val_loss: 0.0455\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0416 - val_loss: 0.0454\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0416 - val_loss: 0.0454\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0416 - val_loss: 0.0453\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0414 - val_loss: 0.0452\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0414 - val_loss: 0.0452\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.0451\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.0450\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.0450\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.0449\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0413 - val_loss: 0.0449\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.0449\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.0448\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0410 - val_loss: 0.0447\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.0446\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0409 - val_loss: 0.0446\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.0408 - val_loss: 0.0445\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0408 - val_loss: 0.0445\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0407 - val_loss: 0.0444\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0407 - val_loss: 0.0444\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0406 - val_loss: 0.0443\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0407 - val_loss: 0.0443\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0406 - val_loss: 0.0442\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 0.0405 - val_loss: 0.0442\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0405 - val_loss: 0.0442\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0404 - val_loss: 0.0442\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.0441\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.0441\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.0440\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.0440\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.0440\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.0440\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0403 - val_loss: 0.0439\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0402 - val_loss: 0.0439\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0402 - val_loss: 0.0439\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0402 - val_loss: 0.0438\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.0402 - val_loss: 0.0438\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.0402 - val_loss: 0.0438\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0401 - val_loss: 0.0438\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0401 - val_loss: 0.0437\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0400 - val_loss: 0.0437\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0400 - val_loss: 0.0437\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0400 - val_loss: 0.0437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x140fb0220>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=10, epochs=50, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04368024319410324"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test) #, batch_size=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 19,  8, 19,  8, 19,  8, 19, 19,  8, 19, 19,  8,  8,  8,  8,  8,\n",
       "        8,  8, 19, 19, 19,  8,  8,  8,  8,  8, 19,  8, 19, 19,  8,  8, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19,  8,  8,  8,  8,  8,  8, 19,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8, 19, 19, 19, 19,  8,  8, 19,\n",
       "        8, 19, 19,  8, 19,  8,  8, 19, 19, 19,  8,  8,  8,  8, 19,  8,  8,\n",
       "        8, 19, 19, 19,  8, 19,  8])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test, batch_size=30)\n",
    "model.predict_classes(X_test, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(y_test, batch_size=30)\n",
    "model.predict_classes(y_test, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10586597 0.10148956 0.10793747 ... 0.09266767 0.0956297  0.11509091]\n",
      " [0.10586597 0.10148956 0.10793747 ... 0.09266767 0.0956297  0.11509091]\n",
      " [0.10334772 0.10641908 0.10192599 ... 0.11288847 0.11110727 0.09710325]\n",
      " ...\n",
      " [0.10334772 0.10641908 0.10192599 ... 0.11288847 0.11110727 0.09710325]\n",
      " [0.11090246 0.09163051 0.11996043 ... 0.05222604 0.06467455 0.15106624]\n",
      " [0.10334772 0.10641908 0.10192599 ... 0.11288847 0.11110727 0.09710325]]\n"
     ]
    }
   ],
   "source": [
    "#Predict values test objects\n",
    "X_new = X_test\n",
    "y_proba = model.predict(X_new) # do I need the .round(2)?\n",
    "print(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "print(len(y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  [[4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [6.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]]\n",
      "y_train:  [[ 0.05      ]\n",
      " [ 0.15918367]\n",
      " [ 0.        ]\n",
      " [ 0.16241497]\n",
      " [ 0.        ]\n",
      " [ 0.15642857]\n",
      " [-0.09270833]\n",
      " [-0.06452991]\n",
      " [ 0.01012397]\n",
      " [ 0.43333333]\n",
      " [-0.02916667]\n",
      " [ 0.109375  ]\n",
      " [ 0.05333333]\n",
      " [ 0.1       ]\n",
      " [-0.01458333]\n",
      " [ 0.0795068 ]\n",
      " [ 0.09      ]\n",
      " [ 0.17857143]\n",
      " [ 0.275     ]\n",
      " [ 0.06009091]\n",
      " [ 0.2625    ]\n",
      " [-0.01916667]\n",
      " [ 0.09166667]\n",
      " [ 0.27559524]\n",
      " [ 0.13318182]\n",
      " [ 0.5       ]\n",
      " [ 0.19622024]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.21785714]\n",
      " [ 0.25      ]\n",
      " [-0.10833333]\n",
      " [ 0.15625   ]\n",
      " [ 0.07666667]\n",
      " [ 0.2625    ]\n",
      " [ 0.22222222]\n",
      " [ 0.0625    ]\n",
      " [-0.08636364]\n",
      " [ 0.18083333]\n",
      " [ 0.        ]\n",
      " [ 0.09970238]\n",
      " [ 0.22166667]\n",
      " [ 0.095     ]\n",
      " [ 0.08848485]\n",
      " [ 0.09333333]\n",
      " [ 0.07238095]\n",
      " [-0.175     ]\n",
      " [ 0.03541667]\n",
      " [ 0.17424242]\n",
      " [ 0.11      ]\n",
      " [ 0.09583333]\n",
      " [ 0.16805556]\n",
      " [ 0.06041667]\n",
      " [ 0.19410173]\n",
      " [ 0.        ]\n",
      " [ 0.28541667]\n",
      " [ 0.25      ]\n",
      " [ 0.16369048]\n",
      " [ 0.14166667]\n",
      " [ 0.21875   ]\n",
      " [ 0.2       ]\n",
      " [ 0.18050964]\n",
      " [ 0.29166667]\n",
      " [ 0.        ]\n",
      " [-0.02588745]\n",
      " [ 0.23224638]\n",
      " [ 0.        ]\n",
      " [ 0.125     ]\n",
      " [ 0.07935606]\n",
      " [ 0.15      ]\n",
      " [ 0.25      ]\n",
      " [ 0.025     ]\n",
      " [ 0.12      ]\n",
      " [-0.078125  ]\n",
      " [-0.071875  ]\n",
      " [-0.08518518]\n",
      " [-1.        ]\n",
      " [ 0.14895833]\n",
      " [ 0.01333333]\n",
      " [ 0.2125    ]\n",
      " [ 0.44      ]\n",
      " [ 0.0125    ]\n",
      " [ 0.06944444]\n",
      " [ 0.12857143]\n",
      " [ 0.04375   ]\n",
      " [ 0.23459208]\n",
      " [ 0.13928571]\n",
      " [ 0.6       ]\n",
      " [ 0.515     ]\n",
      " [-0.13645833]\n",
      " [ 0.25208333]\n",
      " [ 0.28928571]\n",
      " [ 0.13125   ]\n",
      " [ 0.10476191]\n",
      " [ 0.2       ]\n",
      " [ 0.22857143]\n",
      " [ 0.21808036]\n",
      " [ 0.5       ]\n",
      " [ 0.01882716]\n",
      " [ 0.40333333]\n",
      " [ 0.1       ]\n",
      " [ 0.45833333]\n",
      " [ 0.18095238]\n",
      " [ 0.04791667]\n",
      " [ 0.07058823]\n",
      " [ 0.06924242]\n",
      " [ 0.02222222]\n",
      " [ 0.03629704]\n",
      " [-0.15555556]\n",
      " [-0.05      ]\n",
      " [ 0.23579545]\n",
      " [-0.5       ]\n",
      " [ 0.00449495]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.13863636]\n",
      " [-0.034375  ]\n",
      " [ 0.        ]\n",
      " [-0.66666667]\n",
      " [ 0.14583333]\n",
      " [ 0.10871212]\n",
      " [ 0.15659341]\n",
      " [-0.02630386]\n",
      " [-0.00779221]\n",
      " [ 0.6       ]\n",
      " [-0.125     ]\n",
      " [ 0.13333333]\n",
      " [ 0.25      ]\n",
      " [ 0.06966667]\n",
      " [ 0.1       ]\n",
      " [ 0.0375    ]\n",
      " [ 0.00595238]\n",
      " [ 0.01871693]\n",
      " [ 0.10548611]\n",
      " [ 0.259375  ]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.11470058]\n",
      " [ 0.06471861]\n",
      " [ 0.1694697 ]\n",
      " [ 0.20558036]\n",
      " [ 0.52      ]\n",
      " [ 0.06595238]\n",
      " [ 0.075     ]\n",
      " [ 0.22857143]\n",
      " [-0.03333333]\n",
      " [ 0.06666667]\n",
      " [-0.01458333]\n",
      " [ 0.1527972 ]\n",
      " [ 0.11071429]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.05277778]\n",
      " [-0.046875  ]\n",
      " [-0.00333333]\n",
      " [ 0.18229167]\n",
      " [ 0.0476087 ]\n",
      " [ 0.28214286]\n",
      " [ 0.0875    ]\n",
      " [-0.07777778]\n",
      " [ 0.26944444]\n",
      " [ 0.01      ]\n",
      " [-0.04466667]\n",
      " [ 0.14404762]\n",
      " [ 0.45      ]\n",
      " [ 0.09761905]\n",
      " [ 0.175     ]\n",
      " [-0.03333333]\n",
      " [ 0.45      ]\n",
      " [ 0.06428571]\n",
      " [ 0.1       ]\n",
      " [ 0.20767196]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.26346154]\n",
      " [ 0.14      ]\n",
      " [ 0.45833333]\n",
      " [-0.125     ]\n",
      " [ 0.11583333]\n",
      " [ 0.00476431]\n",
      " [ 0.17777778]\n",
      " [ 0.15833333]\n",
      " [-0.14861111]\n",
      " [-0.16666667]\n",
      " [ 0.06944444]\n",
      " [ 0.20857143]\n",
      " [ 0.05404762]\n",
      " [-0.04378157]\n",
      " [ 0.15170068]\n",
      " [ 0.125     ]\n",
      " [ 0.08374242]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.25909091]\n",
      " [ 0.185     ]\n",
      " [ 0.03592558]\n",
      " [ 0.22777778]\n",
      " [ 0.11308081]\n",
      " [ 0.17291667]\n",
      " [-0.25      ]\n",
      " [-0.65      ]\n",
      " [-0.11777778]\n",
      " [ 0.02380952]\n",
      " [ 0.        ]\n",
      " [ 0.30833333]\n",
      " [-0.015     ]\n",
      " [ 0.09393939]\n",
      " [ 0.0625    ]\n",
      " [ 0.13809524]\n",
      " [ 0.15333333]\n",
      " [-0.04356061]\n",
      " [ 0.09402597]\n",
      " [ 0.2147549 ]\n",
      " [ 0.3       ]\n",
      " [ 0.20189349]\n",
      " [ 0.28333333]\n",
      " [-0.35      ]\n",
      " [-0.1075    ]\n",
      " [ 0.45      ]\n",
      " [ 0.10555556]\n",
      " [ 0.3       ]\n",
      " [ 0.07052083]\n",
      " [ 0.1225    ]\n",
      " [ 0.17083333]\n",
      " [ 0.2       ]\n",
      " [ 0.06428571]\n",
      " [ 0.02579365]\n",
      " [-1.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.28583333]\n",
      " [ 0.16176471]\n",
      " [ 0.23333333]\n",
      " [ 0.01369048]\n",
      " [-0.45      ]\n",
      " [ 0.27575758]\n",
      " [ 0.21666667]\n",
      " [ 0.01111111]\n",
      " [ 0.08416667]\n",
      " [-0.01333333]\n",
      " [ 0.19284512]\n",
      " [ 0.16190476]\n",
      " [ 0.01715007]\n",
      " [ 0.01875   ]\n",
      " [ 0.15      ]\n",
      " [ 0.        ]\n",
      " [ 0.04785902]\n",
      " [ 0.20166667]\n",
      " [ 0.17142857]\n",
      " [ 0.28958333]\n",
      " [-0.04523809]\n",
      " [-0.01464646]\n",
      " [ 0.0625    ]\n",
      " [ 0.02166667]\n",
      " [ 0.4       ]\n",
      " [-0.03916667]\n",
      " [ 0.5       ]\n",
      " [-0.55      ]\n",
      " [ 0.02060606]\n",
      " [ 0.10833333]\n",
      " [ 0.14526515]\n",
      " [ 0.275     ]\n",
      " [ 0.16666667]\n",
      " [ 0.55      ]\n",
      " [ 0.05552083]\n",
      " [-0.06372549]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.01277778]\n",
      " [ 0.28888889]\n",
      " [ 0.375     ]\n",
      " [-0.31833333]\n",
      " [ 0.46666667]]\n",
      "X_test:  [[3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]]\n",
      "y_test:  [[ 0.275     ]\n",
      " [ 0.00555556]\n",
      " [ 0.13076923]\n",
      " [ 0.10504329]\n",
      " [ 0.25694444]\n",
      " [ 0.09285714]\n",
      " [-0.04166667]\n",
      " [ 0.125     ]\n",
      " [ 0.09675698]\n",
      " [ 0.5       ]\n",
      " [-0.01186526]\n",
      " [ 0.26666667]\n",
      " [ 0.06435185]\n",
      " [ 0.19027778]\n",
      " [-0.4       ]\n",
      " [ 0.18450635]\n",
      " [ 0.15      ]\n",
      " [ 0.24404762]\n",
      " [ 0.34545455]\n",
      " [-0.025     ]\n",
      " [ 0.2125    ]\n",
      " [ 0.1030303 ]\n",
      " [ 0.44444444]\n",
      " [-0.55555556]\n",
      " [ 0.4       ]\n",
      " [ 0.2765873 ]\n",
      " [ 0.13026245]\n",
      " [ 0.02453704]\n",
      " [ 0.15948162]\n",
      " [ 0.33888889]\n",
      " [ 0.08458333]\n",
      " [ 0.20160256]\n",
      " [ 0.121875  ]\n",
      " [ 0.19820513]\n",
      " [ 0.06428571]\n",
      " [ 0.09419643]\n",
      " [ 0.19888889]\n",
      " [ 0.1537037 ]\n",
      " [ 0.15208333]\n",
      " [ 0.        ]\n",
      " [ 0.38333333]\n",
      " [-0.2       ]\n",
      " [ 0.28571429]\n",
      " [-0.225     ]\n",
      " [ 0.33333333]\n",
      " [-0.29166667]\n",
      " [ 0.05769231]\n",
      " [ 0.01944444]\n",
      " [ 0.675     ]\n",
      " [ 0.09545455]\n",
      " [-0.17638889]\n",
      " [ 0.46666667]\n",
      " [ 0.07967607]\n",
      " [-0.2       ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.06527778]\n",
      " [ 0.21645833]\n",
      " [ 0.2575    ]\n",
      " [ 0.12380952]\n",
      " [ 0.01636905]\n",
      " [ 0.        ]\n",
      " [ 0.18333333]\n",
      " [ 0.25      ]\n",
      " [ 0.1625    ]\n",
      " [ 0.56666667]\n",
      " [ 0.155     ]\n",
      " [ 0.44      ]\n",
      " [ 0.3       ]\n",
      " [-0.01214286]\n",
      " [ 0.07142857]\n",
      " [-0.3625    ]\n",
      " [ 0.0625    ]\n",
      " [-0.31666667]\n",
      " [ 0.47916667]\n",
      " [ 0.06666667]\n",
      " [-0.20833333]\n",
      " [ 0.        ]\n",
      " [ 0.17381245]\n",
      " [ 0.07795056]\n",
      " [-0.14583333]\n",
      " [ 0.03125   ]\n",
      " [ 0.04494949]\n",
      " [ 0.11909091]\n",
      " [ 0.07944444]\n",
      " [-0.0625    ]\n",
      " [-0.1       ]\n",
      " [ 0.01136364]\n",
      " [ 0.10277778]\n",
      " [ 0.02777778]\n",
      " [ 0.        ]\n",
      " [ 0.08863636]]\n"
     ]
    }
   ],
   "source": [
    "# now let's try a prediction based on Q40\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = model_selection.train_test_split(NLPdata[:,1], NLPdata[:,2], train_size=0.75,test_size=0.25, random_state=101)\n",
    "print (\"X_train: \", X_train)\n",
    "print (\"y_train: \", y_train)\n",
    "print (\"X_test: \", X_test)\n",
    "print (\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0437 - val_loss: 0.0457\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.0449\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0416 - val_loss: 0.0443\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.0440\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.0437\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.0435\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0405 - val_loss: 0.0433\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0403 - val_loss: 0.0432\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0402 - val_loss: 0.0431\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0402 - val_loss: 0.0431\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0401 - val_loss: 0.0430\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0400 - val_loss: 0.0430\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0400 - val_loss: 0.0429\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.0429\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0400 - val_loss: 0.0429\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0399 - val_loss: 0.0429\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0400 - val_loss: 0.0428\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0400 - val_loss: 0.0428\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.0399 - val_loss: 0.0428\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0398 - val_loss: 0.0428\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0399 - val_loss: 0.0428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x141022b50>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train2, y_train2, batch_size=10, epochs=50, verbose=1, validation_data=(X_test2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.042753931134939194"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test2, y_test2, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 19, 19,  8, 19, 19, 19, 19,  8, 19, 15, 19, 19, 19, 19, 19, 19,\n",
       "       15, 15, 19,  8,  8, 15, 19,  8,  8, 15,  8, 19, 19, 19,  8, 19, 19,\n",
       "       19, 15, 19, 19, 19,  8, 19, 19, 19, 19, 19,  8, 19, 19,  8, 19, 19,\n",
       "       15, 15, 19, 19, 15,  8,  8,  8, 19, 19, 19,  8,  8, 19, 19, 19, 19,\n",
       "       15, 19,  8, 19, 19,  8, 19, 19, 15, 19, 19,  8, 19,  8, 19, 19, 19,\n",
       "       19, 19, 15, 19, 19,  8,  8])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test2, batch_size=10)\n",
    "model.predict_classes(X_test2, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(y_test2, batch_size=10)\n",
    "model.predict_classes(y_test2, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09921437 0.09913564 0.0992384  ... 0.09890909 0.09881128 0.09928824]\n",
      " [0.09509319 0.09502589 0.09511567 ... 0.094859   0.0947943  0.09516796]\n",
      " [0.09921437 0.09913564 0.0992384  ... 0.09890909 0.09881128 0.09928824]\n",
      " ...\n",
      " [0.09509319 0.09502589 0.09511567 ... 0.094859   0.0947943  0.09516796]\n",
      " [0.11982026 0.11968443 0.11985207 ... 0.11915955 0.11889615 0.11988965]\n",
      " [0.11569908 0.11557467 0.11572934 ... 0.11510947 0.11487918 0.11576936]]\n"
     ]
    }
   ],
   "source": [
    "#Predict values test objects\n",
    "X_new = X_test2\n",
    "y_proba = model.predict(X_new) # do I need the .round(2)?\n",
    "print(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "print(len(y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLPdata = pd.read_csv('nlpdata.csv')\n",
    "NLPdata = NLPdata.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, activation='selu', input_dim=1)) \n",
    "model.add(Dense(28, activation='selu')) \n",
    "#model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  [4. 4. 5. 4. 4. 4. 4. 4. 3. 1. 4. 4. 3. 4. 4. 4. 4. 4. 4. 2. 4. 4. 4. 3.\n",
      " 1. 2. 3. 1. 4. 4. 6. 4. 4. 1. 4. 4. 1. 4. 4. 2. 3. 4. 4. 2. 1. 5. 3. 2.\n",
      " 5. 4. 4. 2. 4. 4. 1. 1. 4. 4. 2. 4. 2. 3. 2. 4. 4. 4. 4. 4. 4. 3. 6. 3.\n",
      " 3. 4. 3. 4. 2. 5. 2. 1. 4. 1. 4. 3. 4. 1. 4. 1. 3. 1. 3. 3. 3. 4. 4. 5.\n",
      " 4. 4. 3. 4. 4. 4. 4. 4. 2. 2. 4. 4. 4. 4. 4. 1. 2. 3. 2. 4. 4. 4. 1. 4.\n",
      " 5. 1. 5. 4. 4. 2. 4. 3. 2. 4. 3. 3. 4. 1. 3. 5. 3. 2. 3. 5. 4. 4. 4. 4.\n",
      " 3. 1. 4. 3. 2. 2. 4. 2. 4. 4. 4. 4. 2. 4. 4. 3. 4. 1. 4. 4. 4. 3. 4. 4.\n",
      " 2. 2. 2. 4. 4. 3. 4. 3. 4. 4. 2. 2. 4. 4. 5. 1. 2. 4. 4. 2. 2. 4. 3. 4.\n",
      " 4. 4. 5. 4. 2. 3. 1. 4. 3. 4. 4. 3. 2. 4. 1. 4. 4. 2. 4. 4. 4. 4. 4. 4.\n",
      " 2. 2. 4. 2. 4. 4. 4. 5. 4. 3. 4. 4. 3. 2. 1. 3. 4. 4. 3. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 1. 4. 4. 3. 1. 3. 4. 4. 4. 4. 4. 4. 2. 4. 2. 3.\n",
      " 3. 4. 2. 4. 4. 4. 4. 4. 1.]\n",
      "y_train:  [ 0.05        0.15918367  0.          0.16241497  0.          0.15642857\n",
      " -0.09270833 -0.06452991  0.01012397  0.43333333 -0.02916667  0.109375\n",
      "  0.05333333  0.1        -0.01458333  0.0795068   0.09        0.17857143\n",
      "  0.275       0.06009091  0.2625     -0.01916667  0.09166667  0.27559524\n",
      "  0.13318182  0.5         0.19622024  0.          0.2         0.21785714\n",
      "  0.25       -0.10833333  0.15625     0.07666667  0.2625      0.22222222\n",
      "  0.0625     -0.08636364  0.18083333  0.          0.09970238  0.22166667\n",
      "  0.095       0.08848485  0.09333333  0.07238095 -0.175       0.03541667\n",
      "  0.17424242  0.11        0.09583333  0.16805556  0.06041667  0.19410173\n",
      "  0.          0.28541667  0.25        0.16369048  0.14166667  0.21875\n",
      "  0.2         0.18050964  0.29166667  0.         -0.02588745  0.23224638\n",
      "  0.          0.125       0.07935606  0.15        0.25        0.025\n",
      "  0.12       -0.078125   -0.071875   -0.08518518 -1.          0.14895833\n",
      "  0.01333333  0.2125      0.44        0.0125      0.06944444  0.12857143\n",
      "  0.04375     0.23459208  0.13928571  0.6         0.515      -0.13645833\n",
      "  0.25208333  0.28928571  0.13125     0.10476191  0.2         0.22857143\n",
      "  0.21808036  0.5         0.01882716  0.40333333  0.1         0.45833333\n",
      "  0.18095238  0.04791667  0.07058823  0.06924242  0.02222222  0.03629704\n",
      " -0.15555556 -0.05        0.23579545 -0.5         0.00449495  0.16666667\n",
      "  0.          0.13863636 -0.034375    0.         -0.66666667  0.14583333\n",
      "  0.10871212  0.15659341 -0.02630386 -0.00779221  0.6        -0.125\n",
      "  0.13333333  0.25        0.06966667  0.1         0.0375      0.00595238\n",
      "  0.01871693  0.10548611  0.259375    0.          0.2         0.11470058\n",
      "  0.06471861  0.1694697   0.20558036  0.52        0.06595238  0.075\n",
      "  0.22857143 -0.03333333  0.06666667 -0.01458333  0.1527972   0.11071429\n",
      "  0.          0.          0.05277778 -0.046875   -0.00333333  0.18229167\n",
      "  0.0476087   0.28214286  0.0875     -0.07777778  0.26944444  0.01\n",
      " -0.04466667  0.14404762  0.45        0.09761905  0.175      -0.03333333\n",
      "  0.45        0.06428571  0.1         0.20767196  0.          0.\n",
      "  0.26346154  0.14        0.45833333 -0.125       0.11583333  0.00476431\n",
      "  0.17777778  0.15833333 -0.14861111 -0.16666667  0.06944444  0.20857143\n",
      "  0.05404762 -0.04378157  0.15170068  0.125       0.08374242  0.16666667\n",
      "  0.          0.25        0.25909091  0.185       0.03592558  0.22777778\n",
      "  0.11308081  0.17291667 -0.25       -0.65       -0.11777778  0.02380952\n",
      "  0.          0.30833333 -0.015       0.09393939  0.0625      0.13809524\n",
      "  0.15333333 -0.04356061  0.09402597  0.2147549   0.3         0.20189349\n",
      "  0.28333333 -0.35       -0.1075      0.45        0.10555556  0.3\n",
      "  0.07052083  0.1225      0.17083333  0.2         0.06428571  0.02579365\n",
      " -1.          0.25        0.28583333  0.16176471  0.23333333  0.01369048\n",
      " -0.45        0.27575758  0.21666667  0.01111111  0.08416667 -0.01333333\n",
      "  0.19284512  0.16190476  0.01715007  0.01875     0.15        0.\n",
      "  0.04785902  0.20166667  0.17142857  0.28958333 -0.04523809 -0.01464646\n",
      "  0.0625      0.02166667  0.4        -0.03916667  0.5        -0.55\n",
      "  0.02060606  0.10833333  0.14526515  0.275       0.16666667  0.55\n",
      "  0.05552083 -0.06372549  0.          0.         -0.01277778  0.28888889\n",
      "  0.375      -0.31833333  0.46666667]\n",
      "X_test:  [3. 3. 4. 2. 4. 3. 4. 2. 3. 4. 3. 3. 5. 4. 4. 4. 4. 4. 4. 2. 1. 2. 4. 4.\n",
      " 4. 4. 4. 1. 4. 2. 2. 4. 4. 3. 1. 2. 2. 1. 2. 2. 1. 2. 3. 4. 4. 4. 4. 4.\n",
      " 4. 2. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 3. 3. 1. 3. 4. 4. 2. 4. 3. 2. 4.\n",
      " 3. 5. 4. 2. 1. 3. 4. 4. 4. 4. 3. 4. 4. 5. 3. 1. 2. 4. 1. 4.]\n",
      "y_test:  [ 0.275       0.00555556  0.13076923  0.10504329  0.25694444  0.09285714\n",
      " -0.04166667  0.125       0.09675698  0.5        -0.01186526  0.26666667\n",
      "  0.06435185  0.19027778 -0.4         0.18450635  0.15        0.24404762\n",
      "  0.34545455 -0.025       0.2125      0.1030303   0.44444444 -0.55555556\n",
      "  0.4         0.2765873   0.13026245  0.02453704  0.15948162  0.33888889\n",
      "  0.08458333  0.20160256  0.121875    0.19820513  0.06428571  0.09419643\n",
      "  0.19888889  0.1537037   0.15208333  0.          0.38333333 -0.2\n",
      "  0.28571429 -0.225       0.33333333 -0.29166667  0.05769231  0.01944444\n",
      "  0.675       0.09545455 -0.17638889  0.46666667  0.07967607 -0.2\n",
      "  0.          0.         -0.06527778  0.21645833  0.2575      0.12380952\n",
      "  0.01636905  0.          0.18333333  0.25        0.1625      0.56666667\n",
      "  0.155       0.44        0.3        -0.01214286  0.07142857 -0.3625\n",
      "  0.0625     -0.31666667  0.47916667  0.06666667 -0.20833333  0.\n",
      "  0.17381245  0.07795056 -0.14583333  0.03125     0.04494949  0.11909091\n",
      "  0.07944444 -0.0625     -0.1         0.01136364  0.10277778  0.02777778\n",
      "  0.          0.08863636]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection as model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(NLPdata[:,0], NLPdata[:,2], train_size=0.75,test_size=0.25, random_state=101)\n",
    "print (\"X_train: \", X_train)\n",
    "print (\"y_train: \", y_train)\n",
    "print (\"X_test: \", X_test)\n",
    "print (\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  [[4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [6.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]]\n",
      "y_train:  [[ 0.05      ]\n",
      " [ 0.15918367]\n",
      " [ 0.        ]\n",
      " [ 0.16241497]\n",
      " [ 0.        ]\n",
      " [ 0.15642857]\n",
      " [-0.09270833]\n",
      " [-0.06452991]\n",
      " [ 0.01012397]\n",
      " [ 0.43333333]\n",
      " [-0.02916667]\n",
      " [ 0.109375  ]\n",
      " [ 0.05333333]\n",
      " [ 0.1       ]\n",
      " [-0.01458333]\n",
      " [ 0.0795068 ]\n",
      " [ 0.09      ]\n",
      " [ 0.17857143]\n",
      " [ 0.275     ]\n",
      " [ 0.06009091]\n",
      " [ 0.2625    ]\n",
      " [-0.01916667]\n",
      " [ 0.09166667]\n",
      " [ 0.27559524]\n",
      " [ 0.13318182]\n",
      " [ 0.5       ]\n",
      " [ 0.19622024]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.21785714]\n",
      " [ 0.25      ]\n",
      " [-0.10833333]\n",
      " [ 0.15625   ]\n",
      " [ 0.07666667]\n",
      " [ 0.2625    ]\n",
      " [ 0.22222222]\n",
      " [ 0.0625    ]\n",
      " [-0.08636364]\n",
      " [ 0.18083333]\n",
      " [ 0.        ]\n",
      " [ 0.09970238]\n",
      " [ 0.22166667]\n",
      " [ 0.095     ]\n",
      " [ 0.08848485]\n",
      " [ 0.09333333]\n",
      " [ 0.07238095]\n",
      " [-0.175     ]\n",
      " [ 0.03541667]\n",
      " [ 0.17424242]\n",
      " [ 0.11      ]\n",
      " [ 0.09583333]\n",
      " [ 0.16805556]\n",
      " [ 0.06041667]\n",
      " [ 0.19410173]\n",
      " [ 0.        ]\n",
      " [ 0.28541667]\n",
      " [ 0.25      ]\n",
      " [ 0.16369048]\n",
      " [ 0.14166667]\n",
      " [ 0.21875   ]\n",
      " [ 0.2       ]\n",
      " [ 0.18050964]\n",
      " [ 0.29166667]\n",
      " [ 0.        ]\n",
      " [-0.02588745]\n",
      " [ 0.23224638]\n",
      " [ 0.        ]\n",
      " [ 0.125     ]\n",
      " [ 0.07935606]\n",
      " [ 0.15      ]\n",
      " [ 0.25      ]\n",
      " [ 0.025     ]\n",
      " [ 0.12      ]\n",
      " [-0.078125  ]\n",
      " [-0.071875  ]\n",
      " [-0.08518518]\n",
      " [-1.        ]\n",
      " [ 0.14895833]\n",
      " [ 0.01333333]\n",
      " [ 0.2125    ]\n",
      " [ 0.44      ]\n",
      " [ 0.0125    ]\n",
      " [ 0.06944444]\n",
      " [ 0.12857143]\n",
      " [ 0.04375   ]\n",
      " [ 0.23459208]\n",
      " [ 0.13928571]\n",
      " [ 0.6       ]\n",
      " [ 0.515     ]\n",
      " [-0.13645833]\n",
      " [ 0.25208333]\n",
      " [ 0.28928571]\n",
      " [ 0.13125   ]\n",
      " [ 0.10476191]\n",
      " [ 0.2       ]\n",
      " [ 0.22857143]\n",
      " [ 0.21808036]\n",
      " [ 0.5       ]\n",
      " [ 0.01882716]\n",
      " [ 0.40333333]\n",
      " [ 0.1       ]\n",
      " [ 0.45833333]\n",
      " [ 0.18095238]\n",
      " [ 0.04791667]\n",
      " [ 0.07058823]\n",
      " [ 0.06924242]\n",
      " [ 0.02222222]\n",
      " [ 0.03629704]\n",
      " [-0.15555556]\n",
      " [-0.05      ]\n",
      " [ 0.23579545]\n",
      " [-0.5       ]\n",
      " [ 0.00449495]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.13863636]\n",
      " [-0.034375  ]\n",
      " [ 0.        ]\n",
      " [-0.66666667]\n",
      " [ 0.14583333]\n",
      " [ 0.10871212]\n",
      " [ 0.15659341]\n",
      " [-0.02630386]\n",
      " [-0.00779221]\n",
      " [ 0.6       ]\n",
      " [-0.125     ]\n",
      " [ 0.13333333]\n",
      " [ 0.25      ]\n",
      " [ 0.06966667]\n",
      " [ 0.1       ]\n",
      " [ 0.0375    ]\n",
      " [ 0.00595238]\n",
      " [ 0.01871693]\n",
      " [ 0.10548611]\n",
      " [ 0.259375  ]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.11470058]\n",
      " [ 0.06471861]\n",
      " [ 0.1694697 ]\n",
      " [ 0.20558036]\n",
      " [ 0.52      ]\n",
      " [ 0.06595238]\n",
      " [ 0.075     ]\n",
      " [ 0.22857143]\n",
      " [-0.03333333]\n",
      " [ 0.06666667]\n",
      " [-0.01458333]\n",
      " [ 0.1527972 ]\n",
      " [ 0.11071429]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.05277778]\n",
      " [-0.046875  ]\n",
      " [-0.00333333]\n",
      " [ 0.18229167]\n",
      " [ 0.0476087 ]\n",
      " [ 0.28214286]\n",
      " [ 0.0875    ]\n",
      " [-0.07777778]\n",
      " [ 0.26944444]\n",
      " [ 0.01      ]\n",
      " [-0.04466667]\n",
      " [ 0.14404762]\n",
      " [ 0.45      ]\n",
      " [ 0.09761905]\n",
      " [ 0.175     ]\n",
      " [-0.03333333]\n",
      " [ 0.45      ]\n",
      " [ 0.06428571]\n",
      " [ 0.1       ]\n",
      " [ 0.20767196]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.26346154]\n",
      " [ 0.14      ]\n",
      " [ 0.45833333]\n",
      " [-0.125     ]\n",
      " [ 0.11583333]\n",
      " [ 0.00476431]\n",
      " [ 0.17777778]\n",
      " [ 0.15833333]\n",
      " [-0.14861111]\n",
      " [-0.16666667]\n",
      " [ 0.06944444]\n",
      " [ 0.20857143]\n",
      " [ 0.05404762]\n",
      " [-0.04378157]\n",
      " [ 0.15170068]\n",
      " [ 0.125     ]\n",
      " [ 0.08374242]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.25909091]\n",
      " [ 0.185     ]\n",
      " [ 0.03592558]\n",
      " [ 0.22777778]\n",
      " [ 0.11308081]\n",
      " [ 0.17291667]\n",
      " [-0.25      ]\n",
      " [-0.65      ]\n",
      " [-0.11777778]\n",
      " [ 0.02380952]\n",
      " [ 0.        ]\n",
      " [ 0.30833333]\n",
      " [-0.015     ]\n",
      " [ 0.09393939]\n",
      " [ 0.0625    ]\n",
      " [ 0.13809524]\n",
      " [ 0.15333333]\n",
      " [-0.04356061]\n",
      " [ 0.09402597]\n",
      " [ 0.2147549 ]\n",
      " [ 0.3       ]\n",
      " [ 0.20189349]\n",
      " [ 0.28333333]\n",
      " [-0.35      ]\n",
      " [-0.1075    ]\n",
      " [ 0.45      ]\n",
      " [ 0.10555556]\n",
      " [ 0.3       ]\n",
      " [ 0.07052083]\n",
      " [ 0.1225    ]\n",
      " [ 0.17083333]\n",
      " [ 0.2       ]\n",
      " [ 0.06428571]\n",
      " [ 0.02579365]\n",
      " [-1.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.28583333]\n",
      " [ 0.16176471]\n",
      " [ 0.23333333]\n",
      " [ 0.01369048]\n",
      " [-0.45      ]\n",
      " [ 0.27575758]\n",
      " [ 0.21666667]\n",
      " [ 0.01111111]\n",
      " [ 0.08416667]\n",
      " [-0.01333333]\n",
      " [ 0.19284512]\n",
      " [ 0.16190476]\n",
      " [ 0.01715007]\n",
      " [ 0.01875   ]\n",
      " [ 0.15      ]\n",
      " [ 0.        ]\n",
      " [ 0.04785902]\n",
      " [ 0.20166667]\n",
      " [ 0.17142857]\n",
      " [ 0.28958333]\n",
      " [-0.04523809]\n",
      " [-0.01464646]\n",
      " [ 0.0625    ]\n",
      " [ 0.02166667]\n",
      " [ 0.4       ]\n",
      " [-0.03916667]\n",
      " [ 0.5       ]\n",
      " [-0.55      ]\n",
      " [ 0.02060606]\n",
      " [ 0.10833333]\n",
      " [ 0.14526515]\n",
      " [ 0.275     ]\n",
      " [ 0.16666667]\n",
      " [ 0.55      ]\n",
      " [ 0.05552083]\n",
      " [-0.06372549]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.01277778]\n",
      " [ 0.28888889]\n",
      " [ 0.375     ]\n",
      " [-0.31833333]\n",
      " [ 0.46666667]]\n",
      "X_test:  [[3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]]\n",
      "y_test:  [[ 0.275     ]\n",
      " [ 0.00555556]\n",
      " [ 0.13076923]\n",
      " [ 0.10504329]\n",
      " [ 0.25694444]\n",
      " [ 0.09285714]\n",
      " [-0.04166667]\n",
      " [ 0.125     ]\n",
      " [ 0.09675698]\n",
      " [ 0.5       ]\n",
      " [-0.01186526]\n",
      " [ 0.26666667]\n",
      " [ 0.06435185]\n",
      " [ 0.19027778]\n",
      " [-0.4       ]\n",
      " [ 0.18450635]\n",
      " [ 0.15      ]\n",
      " [ 0.24404762]\n",
      " [ 0.34545455]\n",
      " [-0.025     ]\n",
      " [ 0.2125    ]\n",
      " [ 0.1030303 ]\n",
      " [ 0.44444444]\n",
      " [-0.55555556]\n",
      " [ 0.4       ]\n",
      " [ 0.2765873 ]\n",
      " [ 0.13026245]\n",
      " [ 0.02453704]\n",
      " [ 0.15948162]\n",
      " [ 0.33888889]\n",
      " [ 0.08458333]\n",
      " [ 0.20160256]\n",
      " [ 0.121875  ]\n",
      " [ 0.19820513]\n",
      " [ 0.06428571]\n",
      " [ 0.09419643]\n",
      " [ 0.19888889]\n",
      " [ 0.1537037 ]\n",
      " [ 0.15208333]\n",
      " [ 0.        ]\n",
      " [ 0.38333333]\n",
      " [-0.2       ]\n",
      " [ 0.28571429]\n",
      " [-0.225     ]\n",
      " [ 0.33333333]\n",
      " [-0.29166667]\n",
      " [ 0.05769231]\n",
      " [ 0.01944444]\n",
      " [ 0.675     ]\n",
      " [ 0.09545455]\n",
      " [-0.17638889]\n",
      " [ 0.46666667]\n",
      " [ 0.07967607]\n",
      " [-0.2       ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.06527778]\n",
      " [ 0.21645833]\n",
      " [ 0.2575    ]\n",
      " [ 0.12380952]\n",
      " [ 0.01636905]\n",
      " [ 0.        ]\n",
      " [ 0.18333333]\n",
      " [ 0.25      ]\n",
      " [ 0.1625    ]\n",
      " [ 0.56666667]\n",
      " [ 0.155     ]\n",
      " [ 0.44      ]\n",
      " [ 0.3       ]\n",
      " [-0.01214286]\n",
      " [ 0.07142857]\n",
      " [-0.3625    ]\n",
      " [ 0.0625    ]\n",
      " [-0.31666667]\n",
      " [ 0.47916667]\n",
      " [ 0.06666667]\n",
      " [-0.20833333]\n",
      " [ 0.        ]\n",
      " [ 0.17381245]\n",
      " [ 0.07795056]\n",
      " [-0.14583333]\n",
      " [ 0.03125   ]\n",
      " [ 0.04494949]\n",
      " [ 0.11909091]\n",
      " [ 0.07944444]\n",
      " [-0.0625    ]\n",
      " [-0.1       ]\n",
      " [ 0.01136364]\n",
      " [ 0.10277778]\n",
      " [ 0.02777778]\n",
      " [ 0.        ]\n",
      " [ 0.08863636]]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1,1)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "X_test = X_test.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print (\"X_train: \", X_train)\n",
    "print (\"y_train: \", y_train)\n",
    "print (\"X_test: \", X_test)\n",
    "print (\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.7368 - val_loss: 0.6176\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5886 - val_loss: 0.5219\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5060 - val_loss: 0.4568\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.4466 - val_loss: 0.4068\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.3998 - val_loss: 0.3670\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 0.3618 - val_loss: 0.3337\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.3297 - val_loss: 0.3053\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.3021 - val_loss: 0.2808\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.2782 - val_loss: 0.2593\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.2571 - val_loss: 0.2406\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.2387 - val_loss: 0.2239\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2222 - val_loss: 0.2092\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2075 - val_loss: 0.1958\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1942 - val_loss: 0.1838\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1823 - val_loss: 0.1731\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1716 - val_loss: 0.1633\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.1618 - val_loss: 0.1545\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1530 - val_loss: 0.1467\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1452 - val_loss: 0.1395\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1380 - val_loss: 0.1330\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1315 - val_loss: 0.1271\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1255 - val_loss: 0.1217\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1201 - val_loss: 0.1168\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.1152 - val_loss: 0.1124\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.1107 - val_loss: 0.1084\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1066 - val_loss: 0.1047\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.1028 - val_loss: 0.1013\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0993 - val_loss: 0.0981\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0960 - val_loss: 0.0951\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0930 - val_loss: 0.0924\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0902 - val_loss: 0.0899\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.0876 - val_loss: 0.0876\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 0.0852 - val_loss: 0.0854\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0830 - val_loss: 0.0834\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0809 - val_loss: 0.0815\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0790 - val_loss: 0.0798\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0772 - val_loss: 0.0782\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0755 - val_loss: 0.0767\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0740 - val_loss: 0.0753\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0740\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0728\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0698 - val_loss: 0.0716\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0686 - val_loss: 0.0705\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0674 - val_loss: 0.0694\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0664 - val_loss: 0.0685\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.0675\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0643 - val_loss: 0.0666\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0634 - val_loss: 0.0658\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0626 - val_loss: 0.0650\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0617 - val_loss: 0.0643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x141098f70>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=10, epochs=50, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06429984420537949"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test) #, batch_size=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27,  1, 27, 27, 27, 27, 27, 27,  1, 27, 27, 27, 27, 27, 27,\n",
       "        1, 27, 27,  1, 27, 27,  1, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,  1, 27, 27, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27,  1, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27,  1, 27, 27,  1, 27])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test, batch_size=30)\n",
    "model.predict_classes(X_test, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 5, 0, 0, 0, 0, 5, 0, 0, 1, 5, 0, 7, 0, 5, 0, 0, 0, 1, 5, 0, 0,\n",
       "       1, 5, 1, 0, 0, 5, 0, 1, 0, 0, 0, 0, 7, 0, 0, 0, 0, 5, 1, 5, 0, 5,\n",
       "       1, 5, 7, 5, 1, 0, 5, 1, 0, 5, 5, 5, 5, 0, 0, 0, 5, 5, 0, 0, 0, 1,\n",
       "       0, 1, 0, 5, 0, 5, 7, 5, 1, 7, 5, 5, 0, 0, 5, 5, 5, 0, 0, 5, 5, 5,\n",
       "       0, 5, 5, 0])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(y_test, batch_size=30)\n",
    "model.predict_classes(y_test, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06744848  0.1094328   0.06484558 ...  0.07242565  0.10734014\n",
      "   0.593116  ]\n",
      " [ 0.06744848  0.1094328   0.06484558 ...  0.07242565  0.10734014\n",
      "   0.593116  ]\n",
      " [ 0.10856145 -0.0245765   0.11785577 ...  0.1157378   0.0824157\n",
      "   0.8399766 ]\n",
      " ...\n",
      " [ 0.10856145 -0.0245765   0.11785577 ...  0.1157378   0.0824157\n",
      "   0.8399766 ]\n",
      " [ 0.07184396  0.17353034 -0.01669979 ...  0.05121145  0.07240693\n",
      "   0.06607196]\n",
      " [ 0.10856145 -0.0245765   0.11785577 ...  0.1157378   0.0824157\n",
      "   0.8399766 ]]\n"
     ]
    }
   ],
   "source": [
    "#Predict values test objects\n",
    "X_new = X_test\n",
    "y_proba = model.predict(X_new) # do I need the .round(2)?\n",
    "print(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "print(len(y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  [[4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [6.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [6.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]]\n",
      "y_train:  [[ 0.05      ]\n",
      " [ 0.15918367]\n",
      " [ 0.        ]\n",
      " [ 0.16241497]\n",
      " [ 0.        ]\n",
      " [ 0.15642857]\n",
      " [-0.09270833]\n",
      " [-0.06452991]\n",
      " [ 0.01012397]\n",
      " [ 0.43333333]\n",
      " [-0.02916667]\n",
      " [ 0.109375  ]\n",
      " [ 0.05333333]\n",
      " [ 0.1       ]\n",
      " [-0.01458333]\n",
      " [ 0.0795068 ]\n",
      " [ 0.09      ]\n",
      " [ 0.17857143]\n",
      " [ 0.275     ]\n",
      " [ 0.06009091]\n",
      " [ 0.2625    ]\n",
      " [-0.01916667]\n",
      " [ 0.09166667]\n",
      " [ 0.27559524]\n",
      " [ 0.13318182]\n",
      " [ 0.5       ]\n",
      " [ 0.19622024]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.21785714]\n",
      " [ 0.25      ]\n",
      " [-0.10833333]\n",
      " [ 0.15625   ]\n",
      " [ 0.07666667]\n",
      " [ 0.2625    ]\n",
      " [ 0.22222222]\n",
      " [ 0.0625    ]\n",
      " [-0.08636364]\n",
      " [ 0.18083333]\n",
      " [ 0.        ]\n",
      " [ 0.09970238]\n",
      " [ 0.22166667]\n",
      " [ 0.095     ]\n",
      " [ 0.08848485]\n",
      " [ 0.09333333]\n",
      " [ 0.07238095]\n",
      " [-0.175     ]\n",
      " [ 0.03541667]\n",
      " [ 0.17424242]\n",
      " [ 0.11      ]\n",
      " [ 0.09583333]\n",
      " [ 0.16805556]\n",
      " [ 0.06041667]\n",
      " [ 0.19410173]\n",
      " [ 0.        ]\n",
      " [ 0.28541667]\n",
      " [ 0.25      ]\n",
      " [ 0.16369048]\n",
      " [ 0.14166667]\n",
      " [ 0.21875   ]\n",
      " [ 0.2       ]\n",
      " [ 0.18050964]\n",
      " [ 0.29166667]\n",
      " [ 0.        ]\n",
      " [-0.02588745]\n",
      " [ 0.23224638]\n",
      " [ 0.        ]\n",
      " [ 0.125     ]\n",
      " [ 0.07935606]\n",
      " [ 0.15      ]\n",
      " [ 0.25      ]\n",
      " [ 0.025     ]\n",
      " [ 0.12      ]\n",
      " [-0.078125  ]\n",
      " [-0.071875  ]\n",
      " [-0.08518518]\n",
      " [-1.        ]\n",
      " [ 0.14895833]\n",
      " [ 0.01333333]\n",
      " [ 0.2125    ]\n",
      " [ 0.44      ]\n",
      " [ 0.0125    ]\n",
      " [ 0.06944444]\n",
      " [ 0.12857143]\n",
      " [ 0.04375   ]\n",
      " [ 0.23459208]\n",
      " [ 0.13928571]\n",
      " [ 0.6       ]\n",
      " [ 0.515     ]\n",
      " [-0.13645833]\n",
      " [ 0.25208333]\n",
      " [ 0.28928571]\n",
      " [ 0.13125   ]\n",
      " [ 0.10476191]\n",
      " [ 0.2       ]\n",
      " [ 0.22857143]\n",
      " [ 0.21808036]\n",
      " [ 0.5       ]\n",
      " [ 0.01882716]\n",
      " [ 0.40333333]\n",
      " [ 0.1       ]\n",
      " [ 0.45833333]\n",
      " [ 0.18095238]\n",
      " [ 0.04791667]\n",
      " [ 0.07058823]\n",
      " [ 0.06924242]\n",
      " [ 0.02222222]\n",
      " [ 0.03629704]\n",
      " [-0.15555556]\n",
      " [-0.05      ]\n",
      " [ 0.23579545]\n",
      " [-0.5       ]\n",
      " [ 0.00449495]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.13863636]\n",
      " [-0.034375  ]\n",
      " [ 0.        ]\n",
      " [-0.66666667]\n",
      " [ 0.14583333]\n",
      " [ 0.10871212]\n",
      " [ 0.15659341]\n",
      " [-0.02630386]\n",
      " [-0.00779221]\n",
      " [ 0.6       ]\n",
      " [-0.125     ]\n",
      " [ 0.13333333]\n",
      " [ 0.25      ]\n",
      " [ 0.06966667]\n",
      " [ 0.1       ]\n",
      " [ 0.0375    ]\n",
      " [ 0.00595238]\n",
      " [ 0.01871693]\n",
      " [ 0.10548611]\n",
      " [ 0.259375  ]\n",
      " [ 0.        ]\n",
      " [ 0.2       ]\n",
      " [ 0.11470058]\n",
      " [ 0.06471861]\n",
      " [ 0.1694697 ]\n",
      " [ 0.20558036]\n",
      " [ 0.52      ]\n",
      " [ 0.06595238]\n",
      " [ 0.075     ]\n",
      " [ 0.22857143]\n",
      " [-0.03333333]\n",
      " [ 0.06666667]\n",
      " [-0.01458333]\n",
      " [ 0.1527972 ]\n",
      " [ 0.11071429]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.05277778]\n",
      " [-0.046875  ]\n",
      " [-0.00333333]\n",
      " [ 0.18229167]\n",
      " [ 0.0476087 ]\n",
      " [ 0.28214286]\n",
      " [ 0.0875    ]\n",
      " [-0.07777778]\n",
      " [ 0.26944444]\n",
      " [ 0.01      ]\n",
      " [-0.04466667]\n",
      " [ 0.14404762]\n",
      " [ 0.45      ]\n",
      " [ 0.09761905]\n",
      " [ 0.175     ]\n",
      " [-0.03333333]\n",
      " [ 0.45      ]\n",
      " [ 0.06428571]\n",
      " [ 0.1       ]\n",
      " [ 0.20767196]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.26346154]\n",
      " [ 0.14      ]\n",
      " [ 0.45833333]\n",
      " [-0.125     ]\n",
      " [ 0.11583333]\n",
      " [ 0.00476431]\n",
      " [ 0.17777778]\n",
      " [ 0.15833333]\n",
      " [-0.14861111]\n",
      " [-0.16666667]\n",
      " [ 0.06944444]\n",
      " [ 0.20857143]\n",
      " [ 0.05404762]\n",
      " [-0.04378157]\n",
      " [ 0.15170068]\n",
      " [ 0.125     ]\n",
      " [ 0.08374242]\n",
      " [ 0.16666667]\n",
      " [ 0.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.25909091]\n",
      " [ 0.185     ]\n",
      " [ 0.03592558]\n",
      " [ 0.22777778]\n",
      " [ 0.11308081]\n",
      " [ 0.17291667]\n",
      " [-0.25      ]\n",
      " [-0.65      ]\n",
      " [-0.11777778]\n",
      " [ 0.02380952]\n",
      " [ 0.        ]\n",
      " [ 0.30833333]\n",
      " [-0.015     ]\n",
      " [ 0.09393939]\n",
      " [ 0.0625    ]\n",
      " [ 0.13809524]\n",
      " [ 0.15333333]\n",
      " [-0.04356061]\n",
      " [ 0.09402597]\n",
      " [ 0.2147549 ]\n",
      " [ 0.3       ]\n",
      " [ 0.20189349]\n",
      " [ 0.28333333]\n",
      " [-0.35      ]\n",
      " [-0.1075    ]\n",
      " [ 0.45      ]\n",
      " [ 0.10555556]\n",
      " [ 0.3       ]\n",
      " [ 0.07052083]\n",
      " [ 0.1225    ]\n",
      " [ 0.17083333]\n",
      " [ 0.2       ]\n",
      " [ 0.06428571]\n",
      " [ 0.02579365]\n",
      " [-1.        ]\n",
      " [ 0.25      ]\n",
      " [ 0.28583333]\n",
      " [ 0.16176471]\n",
      " [ 0.23333333]\n",
      " [ 0.01369048]\n",
      " [-0.45      ]\n",
      " [ 0.27575758]\n",
      " [ 0.21666667]\n",
      " [ 0.01111111]\n",
      " [ 0.08416667]\n",
      " [-0.01333333]\n",
      " [ 0.19284512]\n",
      " [ 0.16190476]\n",
      " [ 0.01715007]\n",
      " [ 0.01875   ]\n",
      " [ 0.15      ]\n",
      " [ 0.        ]\n",
      " [ 0.04785902]\n",
      " [ 0.20166667]\n",
      " [ 0.17142857]\n",
      " [ 0.28958333]\n",
      " [-0.04523809]\n",
      " [-0.01464646]\n",
      " [ 0.0625    ]\n",
      " [ 0.02166667]\n",
      " [ 0.4       ]\n",
      " [-0.03916667]\n",
      " [ 0.5       ]\n",
      " [-0.55      ]\n",
      " [ 0.02060606]\n",
      " [ 0.10833333]\n",
      " [ 0.14526515]\n",
      " [ 0.275     ]\n",
      " [ 0.16666667]\n",
      " [ 0.55      ]\n",
      " [ 0.05552083]\n",
      " [-0.06372549]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.01277778]\n",
      " [ 0.28888889]\n",
      " [ 0.375     ]\n",
      " [-0.31833333]\n",
      " [ 0.46666667]]\n",
      "X_test:  [[3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]]\n",
      "y_test:  [[ 0.275     ]\n",
      " [ 0.00555556]\n",
      " [ 0.13076923]\n",
      " [ 0.10504329]\n",
      " [ 0.25694444]\n",
      " [ 0.09285714]\n",
      " [-0.04166667]\n",
      " [ 0.125     ]\n",
      " [ 0.09675698]\n",
      " [ 0.5       ]\n",
      " [-0.01186526]\n",
      " [ 0.26666667]\n",
      " [ 0.06435185]\n",
      " [ 0.19027778]\n",
      " [-0.4       ]\n",
      " [ 0.18450635]\n",
      " [ 0.15      ]\n",
      " [ 0.24404762]\n",
      " [ 0.34545455]\n",
      " [-0.025     ]\n",
      " [ 0.2125    ]\n",
      " [ 0.1030303 ]\n",
      " [ 0.44444444]\n",
      " [-0.55555556]\n",
      " [ 0.4       ]\n",
      " [ 0.2765873 ]\n",
      " [ 0.13026245]\n",
      " [ 0.02453704]\n",
      " [ 0.15948162]\n",
      " [ 0.33888889]\n",
      " [ 0.08458333]\n",
      " [ 0.20160256]\n",
      " [ 0.121875  ]\n",
      " [ 0.19820513]\n",
      " [ 0.06428571]\n",
      " [ 0.09419643]\n",
      " [ 0.19888889]\n",
      " [ 0.1537037 ]\n",
      " [ 0.15208333]\n",
      " [ 0.        ]\n",
      " [ 0.38333333]\n",
      " [-0.2       ]\n",
      " [ 0.28571429]\n",
      " [-0.225     ]\n",
      " [ 0.33333333]\n",
      " [-0.29166667]\n",
      " [ 0.05769231]\n",
      " [ 0.01944444]\n",
      " [ 0.675     ]\n",
      " [ 0.09545455]\n",
      " [-0.17638889]\n",
      " [ 0.46666667]\n",
      " [ 0.07967607]\n",
      " [-0.2       ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.06527778]\n",
      " [ 0.21645833]\n",
      " [ 0.2575    ]\n",
      " [ 0.12380952]\n",
      " [ 0.01636905]\n",
      " [ 0.        ]\n",
      " [ 0.18333333]\n",
      " [ 0.25      ]\n",
      " [ 0.1625    ]\n",
      " [ 0.56666667]\n",
      " [ 0.155     ]\n",
      " [ 0.44      ]\n",
      " [ 0.3       ]\n",
      " [-0.01214286]\n",
      " [ 0.07142857]\n",
      " [-0.3625    ]\n",
      " [ 0.0625    ]\n",
      " [-0.31666667]\n",
      " [ 0.47916667]\n",
      " [ 0.06666667]\n",
      " [-0.20833333]\n",
      " [ 0.        ]\n",
      " [ 0.17381245]\n",
      " [ 0.07795056]\n",
      " [-0.14583333]\n",
      " [ 0.03125   ]\n",
      " [ 0.04494949]\n",
      " [ 0.11909091]\n",
      " [ 0.07944444]\n",
      " [-0.0625    ]\n",
      " [-0.1       ]\n",
      " [ 0.01136364]\n",
      " [ 0.10277778]\n",
      " [ 0.02777778]\n",
      " [ 0.        ]\n",
      " [ 0.08863636]]\n"
     ]
    }
   ],
   "source": [
    "# now let's try a prediction based on Q40\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = model_selection.train_test_split(NLPdata[:,1], NLPdata[:,2], train_size=0.75,test_size=0.25, random_state=101)\n",
    "print (\"X_train: \", X_train)\n",
    "print (\"y_train: \", y_train)\n",
    "print (\"X_test: \", X_test)\n",
    "print (\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0642 - val_loss: 0.0717\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0638 - val_loss: 0.0712\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0634 - val_loss: 0.0709\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0631 - val_loss: 0.0705\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0627 - val_loss: 0.0701\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0624 - val_loss: 0.0698\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0621 - val_loss: 0.0695\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0618 - val_loss: 0.0692\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0615 - val_loss: 0.0689\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0613 - val_loss: 0.0686\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.0610 - val_loss: 0.0683\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0607 - val_loss: 0.0680\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.0605 - val_loss: 0.0677\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0602 - val_loss: 0.0674\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0600 - val_loss: 0.0672\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0598 - val_loss: 0.0669\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.0667\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.0664\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0591 - val_loss: 0.0662\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0589 - val_loss: 0.0660\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 0.0657\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0585 - val_loss: 0.0655\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0583 - val_loss: 0.0653\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0581 - val_loss: 0.0651\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0648\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0577 - val_loss: 0.0646\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0575 - val_loss: 0.0644\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.0573 - val_loss: 0.0642\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.0572 - val_loss: 0.0640\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 0.0570 - val_loss: 0.0638\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.0568 - val_loss: 0.0636\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 0s 11ms/step - loss: 0.0566 - val_loss: 0.0634\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0565 - val_loss: 0.0632\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0563 - val_loss: 0.0630\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0561 - val_loss: 0.0628\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0560 - val_loss: 0.0626\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0558 - val_loss: 0.0624\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0557 - val_loss: 0.0623\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0555 - val_loss: 0.0621\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0554 - val_loss: 0.0619\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0552 - val_loss: 0.0617\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0551 - val_loss: 0.0616\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0549 - val_loss: 0.0614\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.0548 - val_loss: 0.0612\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0547 - val_loss: 0.0611\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0545 - val_loss: 0.0609\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0544 - val_loss: 0.0608\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0543 - val_loss: 0.0606\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0541 - val_loss: 0.0605\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0540 - val_loss: 0.0603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x140f388b0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train2, y_train2, batch_size=10, epochs=50, verbose=1, validation_data=(X_test2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06030505523085594"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test2, y_test2, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 23,  1, 27,  1, 23,  1, 23, 27,  1, 27, 23, 23,  1,  1, 23, 23,\n",
       "       27, 27, 23, 27, 27, 27,  1, 27, 27, 27, 27, 23,  1, 23, 27,  1,  1,\n",
       "        1, 27,  1,  1, 23, 27, 23,  1, 23, 23,  1, 27, 23,  1, 27,  1,  1,\n",
       "       27, 27, 23, 23, 27, 27, 27, 27, 23,  1, 23, 27, 27,  1, 23, 23,  1,\n",
       "       27, 23, 27, 23, 23, 27, 23,  1, 27, 23, 23, 27,  1, 27, 23, 23, 23,\n",
       "        1,  1, 27,  1, 23, 27, 27])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test2, batch_size=10)\n",
    "model.predict_classes(X_test2, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 23,  0,  0,  0,  0, 23,  0,  0,  1, 23,  0,  0,  0, 12,  0,  0,\n",
       "        0,  0, 23,  0,  0,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0, 23,  0, 12,  0, 12,  0, 12,  0,  0,  1,  0, 12,\n",
       "        1,  0, 12, 23, 23, 23,  0,  0,  0,  0, 23,  0,  0,  0,  1,  0,  1,\n",
       "        0, 23,  0, 12,  0, 12,  1,  0, 12, 23,  0,  0, 12,  0,  0,  0,  0,\n",
       "       23, 23, 23,  0,  0, 23,  0])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(y_test2, batch_size=10)\n",
    "model.predict_classes(y_test2, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08491469  0.18717305  0.01745686 ...  0.05438052  0.09355343\n",
      "   0.04038845]\n",
      " [ 0.1121189   0.05685562  0.02032578 ...  0.03457256  0.0346023\n",
      "  -0.26353198]\n",
      " [ 0.08491469  0.18717305  0.01745686 ...  0.05438052  0.09355343\n",
      "   0.04038845]\n",
      " ...\n",
      " [ 0.1121189   0.05685562  0.02032578 ...  0.03457256  0.0346023\n",
      "  -0.26353198]\n",
      " [ 0.24398406 -0.32391933  0.2188763  ...  0.22207703  0.03203836\n",
      "   1.0890732 ]\n",
      " [ 0.16865304 -0.08433948  0.16712035 ...  0.15048489  0.08398113\n",
      "   0.89674985]]\n"
     ]
    }
   ],
   "source": [
    "#Predict values test objects\n",
    "X_new = X_test2\n",
    "y_proba = model.predict(X_new) # do I need the .round(2)?\n",
    "print(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "print(len(y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1415bb4c0>,\n",
       " <matplotlib.lines.Line2D at 0x1415bb520>,\n",
       " <matplotlib.lines.Line2D at 0x1415bb580>,\n",
       " <matplotlib.lines.Line2D at 0x1415bb640>,\n",
       " <matplotlib.lines.Line2D at 0x1415bb700>,\n",
       " <matplotlib.lines.Line2D at 0x1415bb7c0>,\n",
       " <matplotlib.lines.Line2D at 0x1415bb880>,\n",
       " <matplotlib.lines.Line2D at 0x1415bb940>,\n",
       " <matplotlib.lines.Line2D at 0x1415bba00>,\n",
       " <matplotlib.lines.Line2D at 0x1415bbac0>,\n",
       " <matplotlib.lines.Line2D at 0x1415bbb80>,\n",
       " <matplotlib.lines.Line2D at 0x1415bbc40>,\n",
       " <matplotlib.lines.Line2D at 0x1415bbd00>,\n",
       " <matplotlib.lines.Line2D at 0x1415bbdc0>,\n",
       " <matplotlib.lines.Line2D at 0x1415bbe80>,\n",
       " <matplotlib.lines.Line2D at 0x1415bbf40>,\n",
       " <matplotlib.lines.Line2D at 0x1415c3040>,\n",
       " <matplotlib.lines.Line2D at 0x1415c3100>,\n",
       " <matplotlib.lines.Line2D at 0x1415c31c0>,\n",
       " <matplotlib.lines.Line2D at 0x1415c3280>,\n",
       " <matplotlib.lines.Line2D at 0x1415c3340>,\n",
       " <matplotlib.lines.Line2D at 0x1415c3400>,\n",
       " <matplotlib.lines.Line2D at 0x1415c34c0>,\n",
       " <matplotlib.lines.Line2D at 0x1415c3580>,\n",
       " <matplotlib.lines.Line2D at 0x1415c3640>,\n",
       " <matplotlib.lines.Line2D at 0x1415c3700>,\n",
       " <matplotlib.lines.Line2D at 0x1415c37c0>,\n",
       " <matplotlib.lines.Line2D at 0x1415c3880>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(1,92,92)\n",
    "y1 = y_test2\n",
    "y2 = y_proba\n",
    "plt.figure(figsize=(16,10))\n",
    "#plt.plot(x,y1, 'x')\n",
    "plt.plot(x,y2, '*', c='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14156b490>]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(1,92,92)\n",
    "y1 = y_test2\n",
    "y2 = y_proba\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.plot(x,y1, 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
